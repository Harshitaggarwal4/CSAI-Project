{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_evaluation(vectors_real, vectors_new):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    vectors_real_cpu = vectors_real.detach().cpu().numpy()\n",
    "    vectors_new_cpu = vectors_new.detach().cpu().numpy()\n",
    "\n",
    "    for i in range(vectors_real_cpu.shape[0]):\n",
    "        for j in range(vectors_real_cpu.shape[0]):\n",
    "            if j > i:\n",
    "                errivsi = np.corrcoef(vectors_new_cpu[i,:], vectors_real_cpu[i,:])\n",
    "                errivsj = np.corrcoef(vectors_new_cpu[i,:], vectors_real_cpu[j,:])\n",
    "                errjvsi = np.corrcoef(vectors_new_cpu[j,:], vectors_real_cpu[i,:])\n",
    "                errjvsj = np.corrcoef(vectors_new_cpu[j,:], vectors_real_cpu[j,:])\n",
    "\n",
    "                if (errivsi[0,1] + errjvsj[0,1]) > (errivsj[0,1] + errjvsi[0,1]):\n",
    "                    count += 1\n",
    "                total += 1\n",
    "\n",
    "    accuracy = count / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5(pred, real):\n",
    "    counter = 0.0\n",
    "    counterr = 0.0\n",
    "    counter_ten= 0.0\n",
    "    for i in range(pred.shape[0]):\n",
    "        if np.argmax(pred[i,:])==np.argmax(real[i,:]):\n",
    "            counter+=1\n",
    "        sort = np.flip(np.argsort(pred[i,:]))\n",
    "        holder = np.isin(np.argmax(real[i,:]),sort[:5])\n",
    "        holder_ten = np.isin(np.argmax(real[i,:]),sort[:10])\n",
    "        if holder:\n",
    "            counterr+=1\n",
    "        if holder_ten:\n",
    "            counter_ten+=1\n",
    "    accuracy = counter/pred.shape[0]\n",
    "    accuracy_five = counterr/pred.shape[0]\n",
    "    accuracy_ten = counter_ten/pred.shape[0]\n",
    "    return accuracy, accuracy_five, accuracy_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineProximityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineProximityLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Normalize the input tensors\n",
    "        input1 = F.normalize(input1, p=2, dim=-1)\n",
    "        input2 = F.normalize(input2, p=2, dim=-1)\n",
    "\n",
    "        # Compute the cosine similarity\n",
    "        cosine_sim = torch.sum(input1 * input2, dim=-1)\n",
    "\n",
    "        # Cosine proximity loss is 1 - cosine similarity\n",
    "        loss = 1 - cosine_sim.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanDistanceLoss(nn.Module):\n",
    "    def __init__(self, cosine_loss):\n",
    "        super(MeanDistanceLoss, self).__init__()\n",
    "        self.cosine_loss = cosine_loss\n",
    "        self.val = 179\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        total = 0\n",
    "        total_two = 0\n",
    "        \n",
    "        for i in range((self.val + 1)):\n",
    "            if i == 0:\n",
    "                total += (self.val * self.cosine_loss(y_true, y_pred))\n",
    "            else:\n",
    "                rolled = torch.roll(y_pred, i, dims=0)\n",
    "                total_two -= self.cosine_loss(y_true, rolled)\n",
    "        \n",
    "        return (total_two / self.val) + (total / self.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loader functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROI_loader(subject, fil):\n",
    "    data_path = './data/subjects/'\n",
    "    all_data = sio.loadmat(data_path + subject + '/' + fil)      \n",
    "    ROI = all_data['meta']\n",
    "    Gordon_areas = ROI[0][0][11][0][14]   \n",
    "    try:\n",
    "        data = all_data['examples']\n",
    "    except KeyError:\n",
    "        data = all_data['examples_passagesentences']\n",
    "    return Gordon_areas, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coltocoord_ROI_ordering(subject, fil):\n",
    "    data_path = '../data/subjects/'\n",
    "    all_data = sio.loadmat(data_path + subject + '/' + fil)      \n",
    "    ROI = all_data['meta']\n",
    "    coord = ROI[0][0][5]\n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(area, last_dim, last, Roi_coord):\n",
    "    small = np.sum(last[1])\n",
    "    big = np.sum(last_dim[1])\n",
    "    difference = big-small\n",
    "    assert 0<=difference\n",
    "    assert small == area.shape[1]\n",
    "    if last_dim.shape[1]>last.shape[1]:\n",
    "        helper = np.zeros_like(last_dim)\n",
    "        helper[:,:last.shape[1]] = last\n",
    "        last = helper\n",
    "        checker = 0\n",
    "    else:\n",
    "        checker = last.shape[1] - last_dim.shape[1]\n",
    "        helper = np.zeros_like(last)\n",
    "        helper[:,checker:] = last_dim\n",
    "        last_dim = helper\n",
    "\n",
    "    area_new = np.zeros((area.shape[0],big))\n",
    "    counter = 0\n",
    "    holder = 0\n",
    "    index = 0\n",
    "    index_old = 0\n",
    "    mean = np.zeros((area.shape[0],1))\n",
    "    while counter<last_dim.shape[1]:\n",
    "        if last_dim[1,counter] > (last[1,counter]+holder) and checker == 0:\n",
    "            if last[1,counter] == 0:\n",
    "                if last[1,counter-1] == 0:\n",
    "                    area_new[:,index+last[1,counter]:index+last_dim[1,counter]-holder] = np.tile(mean,(1,last_dim[1,counter] - (last[1,counter]+holder)))\n",
    "                    index+= (last_dim[1,counter]-holder)\n",
    "                else:\n",
    "                    mean = np.reshape(np.mean(area[:,(Roi_coord[:,2] == last[0,counter-1])], axis=1),(area.shape[0],1))\n",
    "                    area_new[:,index+last[1,counter]:index+last_dim[1,counter]-holder] = np.tile(mean,(1,last_dim[1,counter] - (last[1,counter]+holder)))\n",
    "                    index+= (last_dim[1,counter]-holder)\n",
    "            else:\n",
    "                mean = np.reshape(np.mean(area[:,(Roi_coord[:,2] == last[0,counter])], axis=1),(area.shape[0],1))\n",
    "                area_new[:,index:index+last[1,counter]] = area[:,index_old:index_old+last[1,counter]]\n",
    "                if difference > 0 and 0<=(difference - (last_dim[1,counter] - (last[1,counter]+holder))):\n",
    "                    area_new[:,index+last[1,counter]:index+last_dim[1,counter]-holder] = np.tile(mean,(1,last_dim[1,counter] - (last[1,counter]+holder)))\n",
    "                    difference = difference - (last_dim[1,counter] - (last[1,counter]+holder))\n",
    "                    index+= (last_dim[1,counter] - holder)\n",
    "                elif difference > 0 :\n",
    "                    area_new[:,index+last[1,counter]:index+last[1,counter]+difference] = np.tile(mean,(1,difference))\n",
    "                    index += (difference + last[1,counter])\n",
    "                    difference = 0\n",
    "                else:\n",
    "                    index+= last[1,counter]\n",
    "            holder = 0\n",
    "        else:\n",
    "            area_new[:,index:index+last[1,counter]] = area[:,index_old:index_old+last[1,counter]]\n",
    "            index+= last[1,counter]\n",
    "            if checker > 0:\n",
    "                checker-=1\n",
    "                holder = last[1,counter] + holder\n",
    "            else:\n",
    "                holder = (last[1,counter] + holder) - last_dim[1,counter]\n",
    "        index_old+=last[1,counter]\n",
    "\n",
    "        counter+=1\n",
    "    return area_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_sizer(subject):\n",
    "    no_sent = ['M05','M06','M10', 'M13','M16', 'M17']\n",
    "    only_two = ['M03']\n",
    "    only_three = [ 'M08', 'M09','M14']\n",
    "    both = ['P01','M02','M03','M04','M07','M15']\n",
    "    if subject in no_sent:\n",
    "        value = 4530\n",
    "        value_test = 0\n",
    "    if subject in only_two:\n",
    "        value = 4287\n",
    "        value_test = 243\n",
    "    if subject in only_three:\n",
    "        value = 4146\n",
    "        value_test = 384\n",
    "    if subject in both:\n",
    "        value = 3903\n",
    "        value_test = 627\n",
    "    return value, value_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_sentence_word_split_new_matching_all_subjects(subject):\n",
    "    data_path = '../data/subjects/'\n",
    "    vector_path_180 = '../data/glove_data/180_concepts_real.mat'\n",
    "    vector_path_243 = '../data/glove_data/243_sentences_real.mat'\n",
    "    vector_path_384 = '../data/glove_data/384_sentences_real.mat'\n",
    "\n",
    "    vector_180 = sio.loadmat(vector_path_180)['data']\n",
    "    vector_243 = sio.loadmat(vector_path_243)['data']\n",
    "    vector_384 = sio.loadmat(vector_path_384)['data']\n",
    "\n",
    "    subjects = ['P01','M02','M03','M04','M05','M06','M07', 'M08', 'M09','M15','M10', 'M13','M14','M16', 'M17']\n",
    "    no_sent = ['M05','M06','M10', 'M13','M16', 'M17']\n",
    "    only_two = ['M03']\n",
    "    only_three = [ 'M08', 'M09','M14']\n",
    "    both = ['P01','M02','M03','M04','M07','M15']\n",
    "\n",
    "\n",
    "    sizes = np.load('../data/look_ups/sizes.npy')\n",
    "    last_dim_all = np.load('../data/look_ups/last_dim.npz')\n",
    "\n",
    "    if subject in no_sent:\n",
    "        value = 4530\n",
    "        value_test = 0\n",
    "    if subject in only_two:\n",
    "        value = 4287\n",
    "        value_test = 243\n",
    "    if subject in only_three:\n",
    "        value = 4146\n",
    "        value_test = 384\n",
    "    if subject in both:\n",
    "        value = 3903\n",
    "        value_test = 627\n",
    "\n",
    "    data_train = np.zeros((value,65730)) \n",
    "    data_fine = np.zeros((7560,65730)) \n",
    "    data_test = np.zeros((value_test,65730))  \n",
    "    data_fine_test = np.zeros((540,65730))\n",
    "\n",
    "    glove_train = np.zeros((value,300))   \n",
    "    glove_fine = np.zeros((7560,300))   \n",
    "    glove_test = np.zeros((value_test,300))  \n",
    "    glove_fine_test = np.zeros((540,300))\n",
    "\n",
    "    numb = 0\n",
    "    numb_test = 0\n",
    "    numb_fine_test = 0\n",
    "    numb_fine = 0\n",
    "\n",
    "    tot = 0\n",
    "    tot_fine = 0\n",
    "    tot_test= 0\n",
    "    tot_fine_test = 0\n",
    "\n",
    "    for sub in subjects:\n",
    "\n",
    "        folder = os.listdir(data_path +sub)\n",
    "        values = np.zeros((627,212742))     \n",
    "        values_fine = np.zeros((540,212742))\n",
    "        numb_fine_tes = 0\n",
    "        numb_fine1 = 0\n",
    "        numb_tes = 0\n",
    "        numb1 = 0\n",
    "        for fil in folder:\n",
    "            if sub == subject:\n",
    "                if fil.startswith('data_180'):\n",
    "                    Gordon, data = ROI_loader(sub,fil)\n",
    "                    coord = coltocoord_ROI_ordering(sub,fil)\n",
    "                    values_fine[numb_fine_tes:numb_fine_tes+data.shape[0],:data.shape[1]] = data\n",
    "                    if data.shape[0]==180:\n",
    "                        glove_fine_test[numb_fine_test:numb_fine_test+data.shape[0],:] = vector_180\n",
    "                    if fil=='data_180concepts_sentences.mat':\n",
    "                        holder = numb_tes\n",
    "                    numb_fine_test +=data.shape[0]\n",
    "                    numb_fine_tes += data.shape[0]\n",
    "                if fil.startswith('data') and not fil.startswith('data_180'):\n",
    "                    Gordon, data = ROI_loader(sub,fil)\n",
    "                    coord = coltocoord_ROI_ordering(sub,fil)\n",
    "                    values[numb_tes:numb_tes+data.shape[0],:data.shape[1]] = data\n",
    "                    if data.shape[0]==243:\n",
    "                        glove_test[numb_test:numb_test+data.shape[0],:] = vector_243\n",
    "                    if data.shape[0]==384:\n",
    "                        glove_test[numb_test:numb_test+data.shape[0],:] = vector_384\n",
    "                    if fil=='data_180concepts_sentences.mat':\n",
    "                        holder = numb_tes\n",
    "                    numb_test +=data.shape[0]\n",
    "                    numb_tes += data.shape[0]\n",
    "            else:\n",
    "                if fil.startswith('data_180'):\n",
    "                    Gordon, data = ROI_loader(sub,fil)\n",
    "                    coord = coltocoord_ROI_ordering(sub,fil)\n",
    "                    values_fine[numb_fine1:numb_fine1+data.shape[0],:data.shape[1]] = data\n",
    "                    if data.shape[0]==180:\n",
    "                        glove_fine[numb_fine:numb_fine+data.shape[0],:] = vector_180\n",
    "                    numb_fine +=data.shape[0]\n",
    "                    numb_fine1+=data.shape[0]\n",
    "                if fil.startswith('data') and not fil.startswith('data_180'):\n",
    "                    Gordon, data = ROI_loader(sub,fil)\n",
    "                    coord = coltocoord_ROI_ordering(sub,fil)\n",
    "                    values[numb1:numb1+data.shape[0],:data.shape[1]] = data\n",
    "                    if data.shape[0]==243:\n",
    "                        glove_train[numb:numb+data.shape[0],:] = vector_243\n",
    "                    if data.shape[0]==384:\n",
    "                        glove_train[numb:numb+data.shape[0],:] = vector_384\n",
    "                    numb +=data.shape[0]\n",
    "                    numb1+=data.shape[0]\n",
    "        values = values[~(values==0).all(1)]\n",
    "        ind_array = 0\n",
    "        for i in range(333):\n",
    "            helper = True\n",
    "            access = 'arr_' +str(i)\n",
    "            last_dim = last_dim_all[access]\n",
    "            Roi_coord = np.squeeze(coord[Gordon[i][0]])\n",
    "            last = np.asarray(np.unique(Roi_coord[:,2], return_counts=True))\n",
    "            big = np.sum(last_dim[1])\n",
    "            assert big == sizes[i]\n",
    "            indexes = Gordon[i][0]\n",
    "            area = values[:,indexes]\n",
    "            area_fine = values_fine[:,indexes]\n",
    "            if area.shape[0]==0:\n",
    "                helper = False\n",
    "\n",
    "            if sub == subject:\n",
    "\n",
    "                if helper:\n",
    "                    area = np.reshape(area, (values.shape[0],-1))\n",
    "                    area = matcher(area, last_dim, last, Roi_coord)\n",
    "                    data_test[tot_test:tot_test+values.shape[0],ind_array:ind_array+area.shape[1]] = area\n",
    "\n",
    "                area_fine = np.reshape(area_fine, (values_fine.shape[0],-1))\n",
    "                area_fine = matcher(area_fine, last_dim, last, Roi_coord)\n",
    "                data_fine_test[tot_fine_test:tot_fine_test+values_fine.shape[0],ind_array:ind_array+area_fine.shape[1]] = area_fine\n",
    "                ind_array+=sizes[i]\n",
    "            else :\n",
    "                if helper:\n",
    "                    area = np.reshape(area, (values.shape[0],-1))\n",
    "                    area = matcher(area, last_dim, last, Roi_coord)\n",
    "                    data_train[tot:(tot+values.shape[0]),ind_array:ind_array+area.shape[1]] = area\n",
    "\n",
    "                area_fine = np.reshape(area_fine, (values_fine.shape[0],-1))\n",
    "                area_fine = matcher(area_fine, last_dim, last, Roi_coord)\n",
    "                data_fine[tot_fine:(tot_fine+values_fine.shape[0]),ind_array:ind_array+area_fine.shape[1]] = area_fine\n",
    "                ind_array+=sizes[i]\n",
    "        if sub == subject:\n",
    "            tot_test+=values.shape[0]\n",
    "            tot_fine_test+= values_fine.shape[0]\n",
    "        else:\n",
    "            tot+=values.shape[0]\n",
    "            tot_fine+= values_fine.shape[0]\n",
    "\n",
    "    return data_train, data_test, glove_train, glove_test, data_fine, data_fine_test, glove_fine, glove_fine_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'M15' # Selecting the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and testing data\n",
    "with open('../file.pkl', 'rb') as file:\n",
    "    temp_loading_data = pickle.load(file)\n",
    "data_train, data_test, glove_train, glove_test, data_fine, data_fine_test, glove_fine, glove_fine_test = temp_loading_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3903, 65730)\n",
      "(627, 65730)\n",
      "(3903, 300)\n",
      "(627, 300)\n",
      "(7560, 65730)\n",
      "(540, 65730)\n",
      "(7560, 300)\n",
      "(540, 300)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "print(glove_train.shape)\n",
    "print(glove_test.shape)\n",
    "print(data_fine.shape)\n",
    "print(data_fine_test.shape)\n",
    "print(glove_fine.shape)\n",
    "print(glove_fine_test.shape)\n",
    "\n",
    "print(type(glove_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  6 19:05:18 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.86                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   49C    P8              3W /   40W |     467MiB /   8188MiB |     30%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6876    C+G   ...1.0_x64__w2gh52qy24etm\\Nahimic3.exe      N/A      |\n",
      "|    0   N/A  N/A      7148    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE      N/A      |\n",
      "|    0   N/A  N/A      9052    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A      9728    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      9912    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12300    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12324    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16792    C+G   ...on\\124.0.2478.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18356    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     18540    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     19968    C+G   ...973_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     20684    C+G   ...on\\124.0.2478.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     24804    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     24968    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     29392    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     39092    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 180)\n",
      "(7560, 180)\n",
      "(3903, 180)\n"
     ]
    }
   ],
   "source": [
    "value, value_test = class_sizer(subject)\n",
    "class_fine_test =np.eye(180)\n",
    "class_fine = np.reshape(np.tile(class_fine_test,(42,1)),(7560,180))\n",
    "class_fine_test = np.reshape(np.tile(class_fine_test,(3,1)),(540,180)) \n",
    "class_train = np.zeros((3903,180))\n",
    "\n",
    "print(class_fine_test.shape)\n",
    "print(class_fine.shape)\n",
    "print(class_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(data_fine)\n",
    "train_glove_tensor = torch.tensor(glove_fine)\n",
    "train_class_tensor = torch.tensor(class_fine)\n",
    "\n",
    "train_data_tensor = train_data_tensor.to(torch.float32)\n",
    "train_glove_tensor = train_glove_tensor.to(torch.float32)\n",
    "train_class_tensor = train_class_tensor.to(torch.float32)\n",
    "\n",
    "test_data_tensor = torch.tensor(data_fine_test)\n",
    "test_glove_tensor = torch.tensor(glove_fine_test)\n",
    "test_class_tensor = torch.tensor(class_fine_test)\n",
    "\n",
    "test_data_tensor = test_data_tensor.to(torch.float32)\n",
    "test_glove_tensor = test_glove_tensor.to(torch.float32)\n",
    "test_class_tensor = test_class_tensor.to(torch.float32)\n",
    "\n",
    "train_data_tensor = train_data_tensor.to(device)\n",
    "train_glove_tensor = train_glove_tensor.to(device)\n",
    "train_class_tensor = train_class_tensor.to(device)\n",
    "\n",
    "test_data_tensor = test_data_tensor.to(device)\n",
    "test_glove_tensor = test_glove_tensor.to(device)\n",
    "test_class_tensor = test_class_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_train_data_tensor = np.concatenate((data_fine, data_train), axis=0)\n",
    "# reg_train_glove_tensor = np.concatenate((glove_fine, glove_train), axis=0)\n",
    "# reg_train_class_tensor = np.concatenate((class_fine, class_train), axis=0)\n",
    "\n",
    "# reg_test_data_tensor = np.concatenate((data_fine_test, data_test), axis=0)\n",
    "# reg_test_glove_tensor = np.concatenate((glove_fine_test, glove_test), axis=0)\n",
    "# reg_test_class_tensor = np.concatenate((class_fine_test, class_fine_test), axis=0)\n",
    "\n",
    "# reg_train_data_tensor = torch.tensor(reg_train_data_tensor)\n",
    "# reg_train_glove_tensor = torch.tensor(reg_train_glove_tensor)\n",
    "# reg_train_class_tensor = torch.tensor(reg_train_class_tensor)\n",
    "\n",
    "# reg_train_data_tensor = reg_train_data_tensor.to(torch.float32)\n",
    "# reg_train_glove_tensor = reg_train_glove_tensor.to(torch.float32)\n",
    "# reg_train_class_tensor = reg_train_class_tensor.to(torch.float32)\n",
    "\n",
    "# reg_test_data_tensor = torch.tensor(reg_test_data_tensor)\n",
    "# reg_test_glove_tensor = torch.tensor(reg_test_glove_tensor)\n",
    "# reg_test_class_tensor = torch.tensor(reg_test_class_tensor)\n",
    "\n",
    "# reg_test_data_tensor = reg_test_data_tensor.to(torch.float32)\n",
    "# reg_test_glove_tensor = reg_test_glove_tensor.to(torch.float32)\n",
    "# reg_test_class_tensor = reg_test_class_tensor.to(torch.float32)\n",
    "\n",
    "# reg_train_data_tensor = reg_train_data_tensor.to(device)\n",
    "# reg_train_glove_tensor = reg_train_glove_tensor.to(device) \n",
    "# reg_train_class_tensor = reg_train_class_tensor.to(device)\n",
    "\n",
    "# reg_test_data_tensor = reg_test_data_tensor.to(device)   \n",
    "# reg_test_glove_tensor = reg_test_glove_tensor.to(device)\n",
    "# reg_test_class_tensor = reg_test_class_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mean Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "mean_train_data_tensor = torch.tensor(data_train)\n",
    "mean_train_glove_tensor = torch.tensor(glove_train)\n",
    "mean_train_class_tensor = torch.tensor(class_train)\n",
    "\n",
    "mean_train_data_tensor = mean_train_data_tensor.to(torch.float32)\n",
    "mean_train_glove_tensor = mean_train_glove_tensor.to(torch.float32)\n",
    "mean_train_class_tensor = mean_train_class_tensor.to(torch.float32)\n",
    "\n",
    "mean_test_data_tensor = torch.tensor(data_fine_test)\n",
    "mean_test_glove_tensor = torch.tensor(glove_fine_test)\n",
    "mean_test_class_tensor = torch.tensor(class_fine_test)\n",
    "\n",
    "mean_test_data_tensor = mean_test_data_tensor.to(torch.float32)\n",
    "mean_test_glove_tensor = mean_test_glove_tensor.to(torch.float32)\n",
    "mean_test_class_tensor = mean_test_class_tensor.to(torch.float32)\n",
    "\n",
    "mean_train_data_tensor = mean_train_data_tensor.to(device)\n",
    "mean_train_glove_tensor = mean_train_glove_tensor.to(device)\n",
    "mean_train_class_tensor = mean_train_class_tensor.to(device)\n",
    "\n",
    "mean_test_data_tensor = mean_test_data_tensor.to(device)\n",
    "mean_test_glove_tensor = mean_test_glove_tensor.to(device)\n",
    "mean_test_class_tensor = mean_test_class_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecSmallModel(nn.Module):\n",
    "    def __init__(self, rate=0.4, glove_size=300, dense_size=2000, fMRI_size=65730, gordon_areas=333, class_size=180):\n",
    "        super(EncDecSmallModel, self).__init__()\n",
    "        \n",
    "        self.dense_first = nn.Sequential(nn.Linear(fMRI_size, dense_size),nn.LeakyReLU(0.3),nn.BatchNorm1d(dense_size),nn.Dropout(rate))\n",
    "        self.out_glove = nn.Linear(dense_size, glove_size)\n",
    "        self.out_class = nn.Sequential(\n",
    "            nn.Linear(glove_size, class_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_first(x)\n",
    "        out_glove = self.out_glove(x)\n",
    "        out_class = self.out_class(out_glove)\n",
    "        return out_glove, out_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 60/60 [00:58<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Average Loss: 0.0864, Average Top 1 Accuracy: 0.0109, Average Top 5 Accuracy: 0.0453, Average Top 10 Accuracy: 0.0766, Pairwise: 0.7173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:   8%|▊         | 5/60 [00:04<00:54,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m loss_class \u001b[38;5;241m=\u001b[39m loss_fn_class(output_class, batch_class)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_class \u001b[38;5;241m+\u001b[39m loss_glove\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(train_data_tensor, train_glove_tensor, train_class_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_glove_tensor, test_class_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EncDecSmallModel().to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_glove = pairwise_evaluation\n",
    "criterion_class = top_5 \n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 15\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_glove = 0.0\n",
    "    running_loss_class = 0.0\n",
    "    cnt2 = 0\n",
    "    for batch_data, batch_glove, batch_class in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        output_glove, output_class = model(batch_data)\n",
    "        loss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "        loss_class = loss_fn_class(output_class, batch_class)\n",
    "        loss = loss_class + loss_glove\n",
    "        loss.backward()\n",
    "        loss.dtype\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "        running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "        cnt2+=1\n",
    "    \n",
    "    epoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "    epoch_loss_class = running_loss_class / len(train_loader.dataset)\n",
    "    total_a1 = 0\n",
    "    total_a5 = 0\n",
    "    total_a10 = 0\n",
    "    cnt = 0\n",
    "    pair = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_glove, targets_class in test_loader:\n",
    "            outputs_glove, outputs_class = model(inputs)\n",
    "            _, predicted_class = torch.max(outputs_class, 1)\n",
    "            loss_glove = criterion_glove(outputs_glove,targets_glove)\n",
    "\n",
    "            # Calculate average accuracy\n",
    "            a1, a5, a10 = top_5(outputs_class.cpu().numpy(), targets_class.cpu().numpy())\n",
    "            total_a1 += a1\n",
    "            total_a5 += a5\n",
    "            total_a10 += a10\n",
    "            cnt+=1\n",
    "            pair += loss_glove\n",
    "\n",
    "    average_a1 = total_a1 / cnt\n",
    "    average_a5 = total_a5 / cnt\n",
    "    average_a10 = total_a10 / cnt\n",
    "    pair = pair /cnt\n",
    "    running_loss = epoch_loss_class + epoch_loss_glove\n",
    "    average_loss = running_loss / cnt2\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}, Pairwise: {pair:.4f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Big Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecBigModel(nn.Module):\n",
    "    def __init__(self, concating=True):\n",
    "        super(EncDecBigModel, self).__init__()\n",
    "        # Parameters\n",
    "        self.rate = 0.4\n",
    "        self.glove_size = 300\n",
    "        self.fMRI_size = 65730\n",
    "        self.gordon_areas = 333\n",
    "        self.class_size = 180\n",
    "\n",
    "        # Load sizes\n",
    "        sizes = np.load('../data/look_ups/sizes.npy')\n",
    "        reduced = np.load('../data/look_ups/reduced_sizes.npy')\n",
    "\n",
    "        # Small ROI dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        index = 0\n",
    "        for i in range(self.gordon_areas):\n",
    "            new_index = index + sizes[i]\n",
    "            self.dense_layers.append(nn.Linear(sizes[i], reduced[i]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(reduced[i], track_running_stats=True))\n",
    "            index = new_index\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(self.rate)\n",
    "\n",
    "        # Glove layer\n",
    "        self.dense_glove = nn.Linear(sum(reduced), self.glove_size)\n",
    "\n",
    "        # Classification layer\n",
    "        self.dense_class = nn.Linear(self.glove_size, self.class_size)\n",
    "\n",
    "        self.concat = concating\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_outputs = []\n",
    "        index = 0\n",
    "        for i, (layer, bn) in enumerate(zip(self.dense_layers, self.batch_norms)):\n",
    "            small_input = x[:, index:index + layer.in_features]\n",
    "            small_out = F.leaky_relu(layer(small_input), negative_slope=0.3)\n",
    "            small_out = bn(small_out)\n",
    "            branch_outputs.append(small_out)\n",
    "            index += layer.in_features\n",
    "        \n",
    "        con = torch.cat(branch_outputs, dim=1)\n",
    "        con = self.dropout(con)\n",
    "\n",
    "        # Glove layer\n",
    "        out_glove = self.dense_glove(con)\n",
    "\n",
    "        # Classification layer\n",
    "        out_class = F.softmax(self.dense_class(out_glove), dim=1)\n",
    "\n",
    "        if self.concat:\n",
    "            return out_glove, out_class, con\n",
    "        else:\n",
    "            return out_glove, out_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training the big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 42/42 [00:25<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 0.1232, Average Top 1 Accuracy: 0.0130, Average Top 5 Accuracy: 0.0667, Average Top 10 Accuracy: 0.1352, pairwise: 0.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:   2%|▏         | 1/42 [00:01<00:49,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m loss_class \u001b[38;5;241m=\u001b[39m loss_fn_class(output_class, batch_class)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_class \u001b[38;5;241m+\u001b[39m loss_glove\n\u001b[1;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 180\n",
    "train_dataset = TensorDataset(train_data_tensor, train_glove_tensor, train_class_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_glove_tensor, test_class_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EncDecBigModel(concating=False).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_glove = pairwise_evaluation\n",
    "criterion_class = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_glove = 0.0\n",
    "    running_loss_class = 0.0\n",
    "    cnt2 = 0\n",
    "    for batch_data, batch_glove, batch_class in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        output_glove, output_class = model(batch_data)\n",
    "        loss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "        loss_class = loss_fn_class(output_class, batch_class)\n",
    "        loss = loss_class + loss_glove\n",
    "        loss.backward()\n",
    "        loss.dtype\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "        running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "        cnt2+=1\n",
    "    \n",
    "    epoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "    epoch_loss_class = running_loss_class / len(train_loader.dataset)\n",
    "    total_a1 = 0\n",
    "    total_a5 = 0\n",
    "    total_a10 = 0\n",
    "    cnt = 0\n",
    "    pair = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_glove, targets_class in test_loader:\n",
    "            outputs_glove, outputs_class = model(inputs)\n",
    "            _, predicted_class = torch.max(outputs_class, 1)\n",
    "            loss_glove = criterion_glove(outputs_glove,targets_glove)\n",
    "\n",
    "            # Calculate average accuracy\n",
    "            a1, a5, a10 = top_5(outputs_class.cpu().numpy(), targets_class.cpu().numpy())\n",
    "            total_a1 += a1\n",
    "            total_a5 += a5\n",
    "            total_a10 += a10\n",
    "            cnt+=1\n",
    "            pair += loss_glove\n",
    "\n",
    "\n",
    "    average_a1 = total_a1 / cnt\n",
    "    average_a5 = total_a5 / cnt\n",
    "    average_a10 = total_a10 / cnt\n",
    "    pair = pair /cnt\n",
    "    running_loss = epoch_loss_class + epoch_loss_glove\n",
    "    average_loss = running_loss / cnt2\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}, pairwise: {pair:.4f}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, mean = False, training=True):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        rate = 0.0001\n",
    "        dense_size = 200\n",
    "        glove_size = 300\n",
    "        fMRI_size = 65730\n",
    "        reduced_size = 3221\n",
    "        gordon_areas = 333\n",
    "\n",
    "        sizes = np.load('../data/look_ups/sizes.npy')\n",
    "        reduced = np.load('../data/look_ups/reduced_sizes.npy')\n",
    "\n",
    "        self.dense_layers = nn.ModuleList([nn.Linear(sizes[i], reduced[i]) for i in range(len(sizes))])\n",
    "        self.batch_norm = nn.ModuleList([nn.BatchNorm1d(reduced[i]) for i in range(len(sizes))])\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "        self.batch_norm11 = nn.BatchNorm1d(reduced_size)\n",
    "        self.dropout11 = nn.Dropout(rate)\n",
    "\n",
    "        self.dense1 = nn.Linear(reduced_size, dense_size)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(dense_size)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "\n",
    "        self.dense2 = nn.Linear(dense_size, glove_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(glove_size)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        if(training==False):\n",
    "            self.dense2.weight.requires_grad = False  # Freeze the parameters\n",
    "            self.dense2.bias.requires_grad = False\n",
    "        \n",
    "\n",
    "\n",
    "        self.dense3 = nn.Linear(glove_size, 180)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        if(training==False):\n",
    "            self.dense3.weight.requires_grad = False  # Freeze the parameters\n",
    "            self.dense3.bias.requires_grad = False\n",
    "\n",
    "        self.dense4 = nn.Linear(dense_size, reduced_size)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(reduced_size)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "\n",
    "        self.dense_layers_transpose = nn.ModuleList([nn.Linear(reduced[i], sizes[i]) for i in range(len(sizes))])\n",
    "        self.batch_norm4 = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes])\n",
    "\n",
    "        self.mean = mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_outputs = []\n",
    "        index = 0\n",
    "        gordon_areas = 333\n",
    "        sizes = np.load('../data/look_ups/sizes.npy')\n",
    "        reduced = np.load('../data/look_ups/reduced_sizes.npy')\n",
    "        reduced_size = 3221\n",
    "        for i in range(gordon_areas):\n",
    "            new_index = index + sizes[i]\n",
    "            small_input = x[:, index:new_index]\n",
    "            dense_out = self.leaky_relu(self.dense_layers[i](small_input))\n",
    "            dense_out = self.batch_norm[i](dense_out)\n",
    "            dense_out = self.dropout(dense_out)\n",
    "            branch_outputs.append(dense_out)\n",
    "            index = new_index\n",
    "\n",
    "        concat = torch.cat(branch_outputs, dim=1)\n",
    "        dense1_out = self.batch_norm11(concat)\n",
    "        dense1_out = self.dropout11(dense1_out)\n",
    "\n",
    "        out_further = self.leaky_relu(self.dense1(dense1_out))\n",
    "        out_further = self.batch_norm1(out_further)\n",
    "        out_further = self.dropout1(out_further)\n",
    "\n",
    "        out_glove = self.leaky_relu(self.dense2(out_further))\n",
    "        out_glove = self.batch_norm2(out_glove)\n",
    "        out_glove = self.dropout2(out_glove)\n",
    "\n",
    "        out_class = self.softmax(self.dense3(out_glove))\n",
    "\n",
    "        dense4_out = self.leaky_relu(self.dense4(out_further))\n",
    "        dense4_out = self.batch_norm3(dense4_out)\n",
    "        dense4_out = self.dropout3(dense4_out)\n",
    "\n",
    "        branch_outputs1 = []\n",
    "        index1 = 0\n",
    "        for j in range(gordon_areas):\n",
    "            new_index1 = index1 + reduced[j]\n",
    "            small_input = dense4_out[:, index1:new_index1]\n",
    "\n",
    "            dense_out = self.leaky_relu(self.dense_layers_transpose[j](small_input))\n",
    "            dense_out = self.batch_norm4[j](dense_out)\n",
    "            branch_outputs1.append(dense_out)\n",
    "            index1 = new_index1\n",
    "        out = torch.cat(branch_outputs1, dim=1)\n",
    "        \n",
    "        if not self.mean:\n",
    "            return out, out_glove, out_class\n",
    "        else:\n",
    "            return out, out_glove, out_class, concat, dense1_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 180\n",
    "train_dataset = TensorDataset(train_data_tensor, train_glove_tensor, train_class_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_glove_tensor, test_class_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "criterion_glove = pairwise_evaluation\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "\n",
    "epochs = 20\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 42/42 [01:07<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 0.1426, Average Top 1 Accuracy: 0.0167, Average Top 5 Accuracy: 0.0685, Average Top 10 Accuracy: 0.1130, Pairwise: 0.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:   2%|▏         | 1/42 [00:03<02:17,  3.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m loss_autoencoder \u001b[38;5;241m=\u001b[39m loss_fn_autoencoder(batch_data, out)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_class \u001b[38;5;241m+\u001b[39m loss_autoencoder \u001b[38;5;241m+\u001b[39m loss_glove\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_glove = 0.0\n",
    "    running_loss_class = 0.0\n",
    "    running_loss_autoencoder = 0.0\n",
    "    cnt2 = 0\n",
    "    for batch_data, batch_glove, batch_class in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, output_glove, output_class = model(batch_data)\n",
    "        loss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "        loss_class = loss_fn_class(output_class, batch_class)\n",
    "        loss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "        loss = loss_class + loss_autoencoder + loss_glove\n",
    "        loss.backward()\n",
    "        loss.dtype\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "        running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "        running_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "        cnt2+=1\n",
    "    \n",
    "    epoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "    epoch_loss_class = running_loss_class / len(train_loader.dataset)\n",
    "    epoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "    total_a1 = 0\n",
    "    total_a5 = 0\n",
    "    total_a10 = 0\n",
    "    cnt = 0\n",
    "    pair = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_glove, targets_class in test_loader:\n",
    "            out, outputs_glove, outputs_class = model(inputs)\n",
    "            _, predicted_class = torch.max(outputs_class, 1)\n",
    "            loss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "            # Calculate average accuracy\n",
    "            a1, a5, a10 = top_5(outputs_class.cpu().numpy(), targets_class.cpu().numpy())\n",
    "            total_a1 += a1\n",
    "            total_a5 += a5\n",
    "            total_a10 += a10\n",
    "            pair += loss_glove\n",
    "            cnt+=1\n",
    "\n",
    "    average_a1 = total_a1 / cnt\n",
    "    average_a5 = total_a5 / cnt\n",
    "    average_a10 = total_a10 / cnt\n",
    "    running_loss = epoch_loss_class + epoch_loss_glove + epoch_loss_autoencoder\n",
    "    average_loss = running_loss / cnt2\n",
    "    pair = pair /cnt\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}, Pairwise: {pair:.4f}')\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Mean Regularizarion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 180\n",
    "mean_train_dataset = TensorDataset(mean_train_data_tensor, mean_train_glove_tensor, mean_train_class_tensor)\n",
    "mean_train_loader = DataLoader(mean_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mean_test_dataset = TensorDataset(mean_test_data_tensor, mean_test_glove_tensor, mean_test_class_tensor)\n",
    "mean_test_loader = DataLoader(mean_test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Autoencoder(training=False).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "criterion_glove = pairwise_evaluation\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "\n",
    "epochs = 20\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 22/22 [00:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Auto Loss: 2.6741, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 22/22 [00:18<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Auto Loss: 2.4644, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 22/22 [00:17<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Auto Loss: 2.3137, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 22/22 [00:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Auto Loss: 2.2007, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 22/22 [00:15<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Auto Loss: 2.1195, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 22/22 [00:15<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Auto Loss: 2.1038, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 22/22 [00:15<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Auto Loss: 2.0878, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 22/22 [00:15<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Auto Loss: 2.0807, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 22/22 [00:15<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Auto Loss: 2.0566, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 22/22 [00:15<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Auto Loss: 2.0435, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 22/22 [00:15<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Auto Loss: 2.0358, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 22/22 [00:15<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Auto Loss: 2.0355, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 22/22 [00:15<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Auto Loss: 2.0304, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 22/22 [00:15<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Auto Loss: 2.0247, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 22/22 [00:15<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Auto Loss: 2.0233, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 22/22 [00:15<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Auto Loss: 2.0217, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 22/22 [00:15<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Auto Loss: 2.0214, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 22/22 [00:15<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Auto Loss: 2.0209, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 22/22 [00:15<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Auto Loss: 2.0201, )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 22/22 [00:17<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Auto Loss: 2.0191, )\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_glove = 0.0\n",
    "    running_loss_class = 0.0\n",
    "    running_loss_autoencoder = 0.0\n",
    "    cnt2 = 0\n",
    "    for batch_data, batch_glove, batch_class in tqdm.tqdm(mean_train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, output_glove, output_class = model(batch_data)\n",
    "        loss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "        loss_class = loss_fn_class(output_class, batch_class)\n",
    "        loss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "        loss = 0*loss_class + loss_autoencoder + 0*loss_glove\n",
    "        loss.backward()\n",
    "        loss.dtype\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "        running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "        running_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "        cnt2+=1\n",
    "    \n",
    "    epoch_loss_glove = running_loss_glove / len(mean_train_loader.dataset)\n",
    "    epoch_loss_class = running_loss_class / len(mean_train_loader.dataset)\n",
    "    epoch_loss_autoencoder = running_loss_autoencoder / len(mean_train_loader.dataset)\n",
    "\n",
    "    auto_loss_sum = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_glove, targets_class in mean_test_loader:\n",
    "            out, outputs_glove, outputs_class = model(inputs)\n",
    "            auto_loss = loss_fn_autoencoder(inputs, out)\n",
    "\n",
    "            auto_loss_sum += auto_loss\n",
    "            cnt+=1\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    average_a1 = total_a1 / cnt\n",
    "    average_a5 = total_a5 / cnt\n",
    "    average_a10 = total_a10 / cnt\n",
    "    running_loss = epoch_loss_class + epoch_loss_glove + epoch_loss_autoencoder\n",
    "    average_loss = running_loss / cnt2\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Auto Loss: {auto_loss_sum:.4f}, )')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(model.state_dict(), 'pre_autoencoder1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 180\n",
    "train_dataset = TensorDataset(train_data_tensor, train_glove_tensor, train_class_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_glove_tensor, test_class_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Autoencoder(training=True, mean=False).to(device)\n",
    "# load weights\n",
    "model.load_state_dict(torch.load('../pre_autoencoder1.pth'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "criterion_glove = pairwise_evaluation\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "\n",
    "epochs = 20\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 42/42 [00:31<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 0.1332, Average Top 1 Accuracy: 0.0296, Average Top 5 Accuracy: 0.0685, Average Top 10 Accuracy: 0.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 42/42 [00:32<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Average Loss: 0.1302, Average Top 1 Accuracy: 0.0352, Average Top 5 Accuracy: 0.1019, Average Top 10 Accuracy: 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 42/42 [00:34<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Average Loss: 0.1280, Average Top 1 Accuracy: 0.0370, Average Top 5 Accuracy: 0.1148, Average Top 10 Accuracy: 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 42/42 [00:30<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Average Loss: 0.1261, Average Top 1 Accuracy: 0.0352, Average Top 5 Accuracy: 0.1111, Average Top 10 Accuracy: 0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 42/42 [00:29<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Average Loss: 0.1243, Average Top 1 Accuracy: 0.0481, Average Top 5 Accuracy: 0.1426, Average Top 10 Accuracy: 0.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Average Loss: 0.1214, Average Top 1 Accuracy: 0.0500, Average Top 5 Accuracy: 0.1481, Average Top 10 Accuracy: 0.2185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Average Loss: 0.1197, Average Top 1 Accuracy: 0.0704, Average Top 5 Accuracy: 0.1500, Average Top 10 Accuracy: 0.2185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 42/42 [00:29<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Average Loss: 0.1183, Average Top 1 Accuracy: 0.0593, Average Top 5 Accuracy: 0.1630, Average Top 10 Accuracy: 0.2389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Average Loss: 0.1169, Average Top 1 Accuracy: 0.0685, Average Top 5 Accuracy: 0.1574, Average Top 10 Accuracy: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Average Loss: 0.1155, Average Top 1 Accuracy: 0.0685, Average Top 5 Accuracy: 0.1741, Average Top 10 Accuracy: 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Average Loss: 0.1138, Average Top 1 Accuracy: 0.0667, Average Top 5 Accuracy: 0.1667, Average Top 10 Accuracy: 0.2574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Average Loss: 0.1131, Average Top 1 Accuracy: 0.0574, Average Top 5 Accuracy: 0.1611, Average Top 10 Accuracy: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Average Loss: 0.1126, Average Top 1 Accuracy: 0.0556, Average Top 5 Accuracy: 0.1759, Average Top 10 Accuracy: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 42/42 [00:30<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Average Loss: 0.1122, Average Top 1 Accuracy: 0.0593, Average Top 5 Accuracy: 0.1722, Average Top 10 Accuracy: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Average Loss: 0.1118, Average Top 1 Accuracy: 0.0556, Average Top 5 Accuracy: 0.1648, Average Top 10 Accuracy: 0.2481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Average Loss: 0.1112, Average Top 1 Accuracy: 0.0556, Average Top 5 Accuracy: 0.1759, Average Top 10 Accuracy: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 42/42 [00:29<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Average Loss: 0.1111, Average Top 1 Accuracy: 0.0556, Average Top 5 Accuracy: 0.1778, Average Top 10 Accuracy: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Average Loss: 0.1110, Average Top 1 Accuracy: 0.0519, Average Top 5 Accuracy: 0.1722, Average Top 10 Accuracy: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Average Loss: 0.1109, Average Top 1 Accuracy: 0.0574, Average Top 5 Accuracy: 0.1759, Average Top 10 Accuracy: 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Average Loss: 0.1107, Average Top 1 Accuracy: 0.0537, Average Top 5 Accuracy: 0.1722, Average Top 10 Accuracy: 0.2519\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_loss_glove = 0.0\n",
    "    running_loss_class = 0.0\n",
    "    running_loss_autoencoder = 0.0\n",
    "    cnt2 = 0\n",
    "    for batch_data, batch_glove, batch_class in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, output_glove, output_class = model(batch_data)\n",
    "        loss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "        loss_class = loss_fn_class(output_class, batch_class)\n",
    "        loss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "        loss = loss_class + loss_autoencoder + loss_glove\n",
    "        loss.backward()\n",
    "        loss.dtype\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "        running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "        running_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "        cnt2+=1\n",
    "    \n",
    "    epoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "    epoch_loss_class = running_loss_class / len(train_loader.dataset)\n",
    "    epoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "    total_a1 = 0\n",
    "    total_a5 = 0\n",
    "    total_a10 = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets_glove, targets_class in test_loader:\n",
    "            out, outputs_glove, outputs_class = model(inputs)\n",
    "            _, predicted_class = torch.max(outputs_class, 1)\n",
    "\n",
    "            a1, a5, a10 = top_5(outputs_class.cpu().numpy(), targets_class.cpu().numpy())\n",
    "            total_a1 += a1\n",
    "            total_a5 += a5\n",
    "            total_a10 += a10\n",
    "            cnt+=1\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    average_a1 = total_a1 / cnt\n",
    "    average_a5 = total_a5 / cnt\n",
    "    average_a10 = total_a10 / cnt\n",
    "    running_loss = epoch_loss_class + epoch_loss_glove + epoch_loss_autoencoder\n",
    "    average_loss = running_loss / cnt2\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}')\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(model.state_dict(), 'autoencoder1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_Concat(Concat_pred, Concat_mean, first):\n",
    "    out = torch.zeros((180, Concat_pred.shape[1]), dtype=Concat_pred.dtype, device=Concat_pred.device)\n",
    "    for i in range(180):\n",
    "        values = Concat_pred[i::180, :]  # Extracting values from Concat_pred\n",
    "        out[i, :] = torch.mean(values, dim=0)  # Compute mean along dim 0\n",
    "    if first:\n",
    "        return out\n",
    "    else:\n",
    "        return (out + Concat_mean) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(mean, mean_den):\n",
    "    # Calculate the number of repetitions based on the desired shapes\n",
    "    mean_tot_reps = (42, 1)\n",
    "    mean_test_reps = (3, 1)\n",
    "\n",
    "    # Tile and reshape mean tensor\n",
    "    mean_tot = mean.repeat(mean_tot_reps[0], mean_tot_reps[1]).view(-1, mean.shape[1])\n",
    "\n",
    "    # Tile and reshape mean_den tensor\n",
    "    mean_den_tot = mean_den.repeat(mean_tot_reps[0], mean_tot_reps[1]).view(-1, mean_den.shape[1])\n",
    "\n",
    "    # Tile and reshape mean tensor for test\n",
    "    mean_test = mean.repeat(mean_test_reps[0], mean_test_reps[1]).view(-1, mean.shape[1])\n",
    "\n",
    "    # Tile and reshape mean_den tensor for test\n",
    "    mean_den_test = mean_den.repeat(mean_test_reps[0], mean_test_reps[1]).view(-1, mean_den.shape[1])\n",
    "\n",
    "    return mean_tot, mean_den_tot, mean_test, mean_den_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fine_tensor = torch.tensor(data_fine)\n",
    "data_fine_tensor = data_fine_tensor.to(torch.float32)\n",
    "data_fine_tensor = data_fine_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make dataloaders for mean regularization \n",
    "def get_loaders(batch_size, data_train, glove_train, class_train, data_test, glove_test, class_fine_test, mean_tot, mean_den_tot, mean_test, mean_den_test):\n",
    "    # Convert data to tensors\n",
    "    data_train_tensor = torch.tensor(data_train).to(torch.float32).to(device)\n",
    "    class_train_tensor = torch.tensor(class_train).to(torch.float32).to(device)\n",
    "    glove_train_tensor = torch.tensor(glove_train).to(torch.float32).to(device)\n",
    "    mean_tot_tensor = torch.tensor(mean_tot).to(torch.float32).to(device)\n",
    "    mean_den_tot_tensor = torch.tensor(mean_den_tot).to(torch.float32).to(device)\n",
    "    data_test_tensor = torch.tensor(data_test).to(torch.float32).to(device)\n",
    "    class_fine_test_tensor = torch.tensor(class_fine_test).to(torch.float32).to(device)\n",
    "    glove_test_tensor = torch.tensor(glove_test).to(torch.float32).to(device)\n",
    "    mean_test_tensor = torch.tensor(mean_test).to(torch.float32).to(device)\n",
    "    mean_den_test_tensor = torch.tensor(mean_den_test).to(torch.float32).to(device)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(data_train_tensor, class_train_tensor, glove_train_tensor, mean_tot_tensor, mean_den_tot_tensor)\n",
    "    test_dataset = TensorDataset(data_test_tensor, class_fine_test_tensor, glove_test_tensor, mean_test_tensor, mean_den_test_tensor)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Autoencoder(training=True, mean=True).to(device)\n",
    "# load weights\n",
    "model.load_state_dict(torch.load('../autoencoder1.pth'))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn_glove = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "loss_fn_concat = MeanDistanceLoss(CosineProximityLoss())\n",
    "loss_fn_dense = MeanDistanceLoss(CosineProximityLoss())\n",
    "criterion_glove = pairwise_evaluation\n",
    "\n",
    "epochs = 20\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    rec, glove, clas, pred, pred_den = model(data_fine_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mean_Concat(pred, torch.zeros((180, pred.shape[1]), dtype=torch.float32), True)\n",
    "mean_den = mean_Concat(pred_den, torch.zeros((180,pred_den.shape[1]),dtype=torch.float32),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train_model(model, optimizer, loss_fn_glove, loss_fn_class, loss_fn_autoencoder, loss_concat, loss_dense, train_loader, test_loader, epochs, scheduler, mean, mean_den):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_loss_glove = 0.0\n",
    "        running_loss_class = 0.0\n",
    "        running_loss_autoencoder = 0.0\n",
    "        running_loss_concat = 0.0\n",
    "        running_loss_dense = 0.0\n",
    "        cnt2 = 0\n",
    "        for batch_data, batch_class, batch_glove, batch_mean, batch_mean_den in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            rec, glove, clas, pred, pred_den = model(batch_data)\n",
    "            loss_glove = loss_fn_glove(glove, batch_glove)\n",
    "            loss_class = loss_fn_class(clas, batch_class)\n",
    "            loss_autoencoder = loss_fn_autoencoder(batch_data, rec)\n",
    "            loss_concat = loss_fn_concat(pred, batch_mean)\n",
    "            loss_dense = loss_fn_dense(pred_den, batch_mean_den)\n",
    "            loss = loss_class + loss_autoencoder + loss_glove + 2.2*loss_concat + 2.0*loss_dense\n",
    "            loss.backward()\n",
    "            loss.dtype\n",
    "            optimizer.step()\n",
    "            running_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "            running_loss_class += loss_class.item() * batch_data.size(0)\n",
    "            running_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "            running_loss_concat += loss_concat.item() * batch_data.size(0)\n",
    "            running_loss_dense += loss_dense.item() * batch_data.size(0)\n",
    "            cnt2+=1\n",
    "        epoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "        epoch_loss_class = running_loss_class / len(train_loader.dataset)\n",
    "        epoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "        total_a1 = 0\n",
    "        total_a5 = 0\n",
    "        total_a10 = 0\n",
    "        cnt = 0\n",
    "        pair = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets_class, targets_glove, mean, mean_den in test_loader:\n",
    "                rec, glove, clas, pred, pred_den = model(inputs)\n",
    "                loss_glove = criterion_glove(glove,targets_glove)\n",
    "                loss_class = loss_fn_class(clas, targets_class)\n",
    "                loss_autoencoder = loss_fn_autoencoder(inputs, rec)\n",
    "                loss_concat = loss_fn_concat(pred, mean)\n",
    "                loss_dense = loss_fn_dense(pred_den, mean_den)\n",
    "                \n",
    "                # Calculate average accuracy\n",
    "                a1, a5, a10 = top_5(clas.cpu().numpy(), targets_class.cpu().numpy())\n",
    "                total_a1 += a1\n",
    "                total_a5 += a5\n",
    "                total_a10 += a10\n",
    "                pair += loss_glove\n",
    "                cnt+=1\n",
    "        \n",
    "        pair = pair /cnt\n",
    "        average_a1 = total_a1 / cnt\n",
    "        average_a5 = total_a5 / cnt\n",
    "        average_a10 = total_a10 / cnt\n",
    "        running_loss = epoch_loss_class + epoch_loss_glove + epoch_loss_autoencoder\n",
    "        average_loss = running_loss / cnt2\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}, Pairwise: {pair:.4f}')\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_37644\\2506654132.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_tot_tensor = torch.tensor(mean_tot).to(torch.float32).to(device)\n",
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_37644\\2506654132.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_den_tot_tensor = torch.tensor(mean_den_tot).to(torch.float32).to(device)\n",
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_37644\\2506654132.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_test_tensor = torch.tensor(mean_test).to(torch.float32).to(device)\n",
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_37644\\2506654132.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_den_test_tensor = torch.tensor(mean_den_test).to(torch.float32).to(device)\n",
      "Epoch 1/5: 100%|██████████| 60/60 [02:15<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 0.0809, Average Top 1 Accuracy: 0.0643, Average Top 5 Accuracy: 0.1708, Average Top 10 Accuracy: 0.2469, Pairwise: 0.8152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 60/60 [02:14<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 0.0793, Average Top 1 Accuracy: 0.0690, Average Top 5 Accuracy: 0.1583, Average Top 10 Accuracy: 0.2397, Pairwise: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 60/60 [02:14<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 0.0778, Average Top 1 Accuracy: 0.0634, Average Top 5 Accuracy: 0.1817, Average Top 10 Accuracy: 0.2451, Pairwise: 0.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 60/60 [02:06<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 0.0766, Average Top 1 Accuracy: 0.0556, Average Top 5 Accuracy: 0.1926, Average Top 10 Accuracy: 0.2520, Pairwise: 0.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 60/60 [02:05<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Average Loss: 0.0759, Average Top 1 Accuracy: 0.0509, Average Top 5 Accuracy: 0.1511, Average Top 10 Accuracy: 0.2422, Pairwise: 0.7858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 60/60 [03:16<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 0.0736, Average Top 1 Accuracy: 0.0627, Average Top 5 Accuracy: 0.1879, Average Top 10 Accuracy: 0.2679, Pairwise: 0.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   5%|▌         | 3/60 [00:11<03:30,  3.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m rec, glove, clas, pred, pred_den \u001b[38;5;241m=\u001b[39m model(data_fine_tensor)\n\u001b[0;32m      4\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m get_loaders(\u001b[38;5;241m128\u001b[39m, data_fine, glove_fine, class_fine, data_fine_test, glove_fine_test, class_fine_test, mean_tot, mean_den_tot, mean_test, mean_den_test)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_glove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_dense\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_den\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, loss_fn_glove, loss_fn_class, loss_fn_autoencoder, loss_concat, loss_dense, train_loader, test_loader, epochs, scheduler, mean, mean_den)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss_autoencoder \u001b[38;5;241m=\u001b[39m loss_fn_autoencoder(batch_data, rec)\n\u001b[0;32m     18\u001b[0m loss_concat \u001b[38;5;241m=\u001b[39m loss_fn_concat(pred, batch_mean)\n\u001b[1;32m---> 19\u001b[0m loss_dense \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_den\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_mean_den\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_class \u001b[38;5;241m+\u001b[39m loss_autoencoder \u001b[38;5;241m+\u001b[39m loss_glove \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.2\u001b[39m\u001b[38;5;241m*\u001b[39mloss_concat \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39mloss_dense\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mMeanDistanceLoss.forward\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m         rolled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(y_pred, i, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m         total_two \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrolled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (total_two \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval) \u001b[38;5;241m+\u001b[39m (total \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mCosineProximityLoss.forward\u001b[1;34m(self, input1, input2)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Normalize the input tensors\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     input1 \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     input2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(input2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Compute the cosine similarity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:4754\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4753\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(p, dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mclamp_min(eps)\u001b[38;5;241m.\u001b[39mexpand_as(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 4754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4756\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(p, dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mclamp_min_(eps)\u001b[38;5;241m.\u001b[39mexpand_as(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    mean_tot, mean_den_tot, mean_test, mean_den_test = get_tensors(mean, mean_den)\n",
    "    rec, glove, clas, pred, pred_den = model(data_fine_tensor)\n",
    "    train_loader, test_loader = get_loaders(128, data_fine, glove_fine, class_fine, data_fine_test, glove_fine_test, class_fine_test, mean_tot, mean_den_tot, mean_test, mean_den_test)\n",
    "    train_model(model, optimizer, loss_fn_glove, loss_fn_class, loss_fn_autoencoder, loss_fn_concat, loss_fn_dense, train_loader, test_loader, epochs, scheduler, mean, mean_den)\n",
    "    model.eval()\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        rec, glove, clas, pred, pred_den = model(data_fine_tensor)\n",
    "    mean = mean_Concat(pred, mean, False)\n",
    "    mean_den = mean_Concat(pred_den, mean_den, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
