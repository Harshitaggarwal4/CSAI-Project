{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fmri to Clip Representation\n",
    "\n",
    "\n",
    "## MICS\n",
    "\n",
    "---\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as p\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import generate_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the random seed to a specific value, for example, 42\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_evaluation(vectors_real, vectors_new):\n",
    "\tcount = 0\n",
    "\ttotal = 0\n",
    "\tvectors_real_cpu = vectors_real.detach().cpu().numpy()\n",
    "\tvectors_new_cpu = vectors_new.detach().cpu().numpy()\n",
    "\n",
    "\tfor i in range(vectors_real_cpu.shape[0]):\n",
    "\t\tfor j in range(vectors_real_cpu.shape[0]):\n",
    "\t\t\tif j > i:\n",
    "\t\t\t\terrivsi = np.corrcoef(vectors_new_cpu[i,:], vectors_real_cpu[i,:])\n",
    "\t\t\t\terrivsj = np.corrcoef(vectors_new_cpu[i,:], vectors_real_cpu[j,:])\n",
    "\t\t\t\terrjvsi = np.corrcoef(vectors_new_cpu[j,:], vectors_real_cpu[i,:])\n",
    "\t\t\t\terrjvsj = np.corrcoef(vectors_new_cpu[j,:], vectors_real_cpu[j,:])\n",
    "\n",
    "\t\t\t\tif (errivsi[0,1] + errjvsj[0,1]) > (errivsj[0,1] + errjvsi[0,1]):\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\t\ttotal += 1\n",
    "\n",
    "\taccuracy = count / total\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5(pred, real):\n",
    "\tcounter = 0.0\n",
    "\tcounterr = 0.0\n",
    "\tcounter_ten= 0.0\n",
    "\tfor i in range(pred.shape[0]):\n",
    "\t\tif np.argmax(pred[i,:])==np.argmax(real[i,:]):\n",
    "\t\t\tcounter+=1\n",
    "\t\tsort = np.flip(np.argsort(pred[i,:]))\n",
    "\t\tholder = np.isin(np.argmax(real[i,:]),sort[:5])\n",
    "\t\tholder_ten = np.isin(np.argmax(real[i,:]),sort[:10])\n",
    "\t\tif holder:\n",
    "\t\t\tcounterr+=1\n",
    "\t\tif holder_ten:\n",
    "\t\t\tcounter_ten+=1\n",
    "\taccuracy = counter/pred.shape[0]\n",
    "\taccuracy_five = counterr/pred.shape[0]\n",
    "\taccuracy_ten = counter_ten/pred.shape[0]\n",
    "\treturn accuracy, accuracy_five, accuracy_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineProximityLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CosineProximityLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, input1, input2):\n",
    "\t\t# Normalize the input tensors\n",
    "\t\tinput1 = F.normalize(input1, p=2, dim=-1)\n",
    "\t\tinput2 = F.normalize(input2, p=2, dim=-1)\n",
    "\n",
    "\t\t# Compute the cosine similarity\n",
    "\t\tcosine_sim = torch.sum(input1 * input2, dim=-1)\n",
    "\n",
    "\t\t# Cosine proximity loss is 1 - cosine similarity\n",
    "\t\tloss = 1 - cosine_sim.mean()\n",
    "\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MSELoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, input1, input2):\n",
    "\t\tmse_loss = torch.mean((input1 - input2)**2)\n",
    "\n",
    "\t\treturn mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanDistanceLoss(nn.Module):\n",
    "\tdef __init__(self, cosine_loss):\n",
    "\t\tsuper(MeanDistanceLoss, self).__init__()\n",
    "\t\tself.cosine_loss = cosine_loss\n",
    "\t\tself.val = 179\n",
    "\n",
    "\tdef forward(self, y_true, y_pred):\n",
    "\t\ttotal = 0\n",
    "\t\ttotal_two = 0\n",
    "\t\t\n",
    "\t\tfor i in range((self.val + 1)):\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\ttotal += (self.val * self.cosine_loss(y_true, y_pred))\n",
    "\t\t\telse:\n",
    "\t\t\t\trolled = torch.roll(y_pred, i, dims=0)\n",
    "\t\t\t\ttotal_two -= self.cosine_loss(y_true, rolled)\n",
    "\t\t\n",
    "\t\treturn (total_two / self.val) + (total / self.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Data from BOLD5000\n",
    "\n",
    "### Getting the fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_CSI(patient_id):\n",
    "\troot_path = 'Stimuli_Presentation_Lists/'\n",
    "\tpath = os.path.join(root_path, f'CSI{patient_id}/')\n",
    "\n",
    "\tlist_images_names = []\n",
    "\ttotal_images = 0\n",
    "\n",
    "\t# Get subdirectory paths\n",
    "\tsub_folder_paths = glob.glob(os.path.join(path, f'CSI{patient_id}_sess*/'))\n",
    "\tsub_folder_paths = sub_folder_paths  # Remove the last item\n",
    "\n",
    "\t# Loop through each subdirectory\n",
    "\tfor i, folder in enumerate(sub_folder_paths):\n",
    "\t\tindex = i + 1\n",
    "\t\tfile_paths = glob.glob(os.path.join(folder, f'*.txt'))\n",
    "\t\t# print(file_paths)\n",
    "  \n",
    "\t\t# raise Exception('Stop here')\n",
    "\t\tfor file_path in file_paths:\n",
    "\t\t\tdata_from_file = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "\t\t\ttotal_images += len(data_from_file)\n",
    "\t\t\t# get list of images \n",
    "\t\t\timages = data_from_file[0].values\n",
    "\t\t\t# append to list of images\n",
    "\t\t\tlist_images_names.extend(images)\n",
    "\n",
    "\treturn list_images_names\n",
    "\n",
    "list_images_names_CSI1 = get_images_from_CSI(1)\n",
    "\n",
    "# print(\"Total images:\", list_images_names_CSI1)\n",
    "print(\"Length of list_images_names:\", len(list_images_names_CSI1))\n",
    "\n",
    "list_images_names = list_images_names_CSI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images file path\n",
    "coco_images_path = \"./Scene_Stimuli/Original_Images/COCO\"\n",
    "\n",
    "# loading all list of images names \n",
    "coco_images = glob.glob(coco_images_path + '/*')\n",
    "\n",
    "# load images\n",
    "def load_image(image_path):\n",
    "\t# load rgb image\n",
    "\timage = Image.open(image_path)\n",
    "\t\n",
    "\t# convert to numpy array\n",
    "\timage = np.array(image)\n",
    "\t\n",
    "\tif len(image.shape) == 2:\n",
    "\t\timage = np.stack((image,)*3, axis=-1)\n",
    "\t\n",
    "\tassert image.shape[2] == 3, \"Image should have 3 channels\"\n",
    "\t\n",
    "\treturn image\n",
    "\n",
    "def load_images_from_names(image_name):\n",
    "\ttry:\n",
    "\t\tif image_name.startswith('COCO'):\n",
    "\t\t\timage_path = coco_images_path + '/' + image_name\n",
    "\t\t\treturn load_image(image_path)\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"Image not of coco dataset\")\n",
    "\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error loading image:\", image_name, e)\n",
    "  \n",
    "\t\treturn None\n",
    "\n",
    "list_images_names_CSI1 = get_images_from_CSI(1)\n",
    "\n",
    "# print(\"Total images:\", list_images_names_CSI1)\n",
    "print(\"Length of list_images_names:\", len(list_images_names_CSI1))\n",
    "\n",
    "list_images_names = list_images_names_CSI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(list_images_names):\n",
    "\tmask = np.zeros(len(list_images_names))\n",
    "\tseen_images = set()\n",
    "\n",
    "\tfor i, image in enumerate(list_images_names):\n",
    "\t\t# image name is unique\n",
    "\t\timage_name = image.split('.')[0]\n",
    "\n",
    "\t\tif image_name.startswith('rep'):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif image_name in seen_images:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif not image_name.startswith('COCO'):\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tmask[i] = 1\n",
    "\t\tseen_images.add(image_name)\n",
    "\n",
    "\treturn mask\n",
    "\n",
    "# Example usage:\n",
    "mask = get_mask(list_images_names)\n",
    "\t\n",
    "print(\"Unique images:\", np.sum(mask))\n",
    "print(\"len of mask:\", len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mask on list_images_names\n",
    "unique_images = [image for i, image in enumerate(list_images_names) if mask[i] == 1]\n",
    "\n",
    "print(\"Length of unique images:\", len(unique_images))\n",
    "\n",
    "# unique_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images file path\n",
    "coco_images_path = \"./Scene_Stimuli/Original_Images/COCO\"\n",
    "\n",
    "# loading all list of images names \n",
    "coco_images = glob.glob(coco_images_path + '/*')\n",
    "# imgnet_images = glob.glob(imgnet_images_path + '/*')\n",
    "# scene_images = glob.glob(scene_images_path + '/*')\n",
    "\n",
    "# load images\n",
    "def load_image(image_path):\n",
    "\t# load rgb image\n",
    "\timage = Image.open(image_path)\n",
    "\t\n",
    "\t# convert to numpy array\n",
    "\timage = np.array(image)\n",
    "\t\n",
    "\tif len(image.shape) == 2:\n",
    "\t\timage = np.stack((image,)*3, axis=-1)\n",
    "\t\n",
    "\tassert image.shape[2] == 3, \"Image should have 3 channels\"\n",
    "\t\n",
    "\treturn image\n",
    "\n",
    "def load_images_from_names(image_name):\n",
    "\ttry:\n",
    "\t\tif image_name.startswith('COCO'):\n",
    "\t\t\timage_path = coco_images_path + '/' + image_name\n",
    "\t\t\treturn load_image(image_path)\n",
    "\t\telse:\n",
    "\t\t\traise Exception(\"Image not of coco dataset\")\n",
    "\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error loading image:\", image_name, e)\n",
    "  \n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Get clip features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the CLIP model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clip_image_activations(input_images):\n",
    "\tfeature_vectors = []\n",
    "\tfor img_path in tqdm.tqdm(input_images):\n",
    "\t\t# Load and preprocess the image\n",
    "\t\timage = load_images_from_names(img_path)\n",
    "\t\timage = Image.fromarray(np.uint8(image))\n",
    "\t\t# Preprocess the image\n",
    "\t\timage = preprocess(image).unsqueeze(0).to(device)\n",
    "\t\t\n",
    "\t\t# Generate features\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Pass the image input to the image encoder\n",
    "\t\t\timage_features = model.encode_image(image)\n",
    "\t\t\t# print shape \n",
    "\t\t\tfeature_vectors.append(image_features[0])\n",
    "\t\t\n",
    "\treturn feature_vectors\n",
    "\n",
    "# Example usage\n",
    "ex_input_images = [\"COCO_train2014_000000475840.jpg\"]\n",
    "ex_image_features = generate_clip_image_activations(ex_input_images)\n",
    "print(ex_image_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for all the unique images\n",
    "unique_image_features = generate_clip_image_activations(unique_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack tensor\n",
    "tensor_image_features = torch.stack(unique_image_features)\n",
    "tensor_image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump variables data  \n",
    "pickle.dump(unique_image_features, open(\"./data/unique_image_features.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vairable \n",
    "unique_images_features = pickle.load(open(\"./data/unique_image_features.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the captions adn representations\n",
    "# load pickle file\n",
    "with open('data\\caption_to_feature.pkl','rb') as f:\n",
    "\tcaption_to_feature = pickle.load(f)\n",
    " \n",
    "# load pickle file \n",
    "with open(\"data/img_best_caption.pkl\",'rb') as f:\n",
    "\timg_best_caption = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_caption = {}\n",
    "\n",
    "for k,v in img_best_caption.items():\n",
    "\timg_name_to_caption[k.split('.')[0]] = v\n",
    " \n",
    "print(list(img_name_to_caption.values())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_image_features = torch.stack(unique_images_features)\n",
    "\n",
    "unique_captions = list(caption_to_feature.keys())\n",
    "unique_captions_features = list(caption_to_feature.values())\n",
    "\n",
    "len(unique_captions),len(unique_captions_features), len(unique_images_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Apply Mask to FMRI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_data_path = 'data/CSI1/mat/CSI1_ROIs_TR34.mat'\n",
    "\n",
    "# get the ROI data\n",
    "from scipy.io import loadmat\n",
    "roi_data = loadmat(roi_data_path)\n",
    "\n",
    "print(\"ROI data keys:\", roi_data.keys())\n",
    "\n",
    "# delete columns that start with __\n",
    "roi_data = {k: v for k, v in roi_data.items() if not k.startswith('__')}\n",
    "\n",
    "print(\"ROI data keys:\", roi_data.keys())\n",
    "roi_data['RHLOC'].shape\n",
    "\n",
    "# sum of voxels in each ROI\n",
    "roi_data_sum = 0 \n",
    "\n",
    "for roi in roi_data:\n",
    "\troi_data_sum += roi_data[roi].shape[1]\n",
    " \n",
    "print(\"Total number of voxels in all ROIs:\", roi_data_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mask on roi_data\n",
    "def apply_mask_on_roi_data(roi_data, mask):\n",
    "\troi_data_masked = {}\n",
    "\tfor roi in roi_data:\n",
    "\t\troi_data_masked[roi] = roi_data[roi][mask == 1,:]\n",
    "\t\n",
    "\treturn roi_data_masked\n",
    "\n",
    "# apply mask on roi_data\n",
    "roi_data_masked = apply_mask_on_roi_data(roi_data, mask)\n",
    "\n",
    "# sum of voxels in each ROI\n",
    "roi_data_sum = 0\n",
    "for roi in roi_data_masked:\n",
    "\t# print shape of roi\n",
    "\tprint(f\"ROI: {roi}, Shape: {roi_data_masked[roi].shape}\")\n",
    "\troi_data_sum += roi_data_masked[roi].shape[1]\n",
    " \n",
    "print(\"Total number of voxels in all ROIs after applying mask:\", roi_data_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the roi voxels\n",
    "def concat_roi_voxels(roi_data):\n",
    "\troi_data_concat = np.concatenate([roi_data[roi] for roi in roi_data], axis=1)\n",
    "\treturn roi_data_concat\n",
    "\n",
    "roi_data_concat = concat_roi_voxels(roi_data_masked)\n",
    "# convert to tensor\n",
    "roi_data_concat = torch.tensor(roi_data_concat, dtype=torch.float32)\n",
    "print(\"Shape of roi_data_concat:\", roi_data_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making new npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizess = []\n",
    "for roi in roi_data_masked:\n",
    "\tsizess.append(roi_data_masked[roi].shape[1])\n",
    "print(sizess)\n",
    "np.save('./data/look_ups/sizes_clip.npy', sizess)\n",
    "sizess = np.array(sizess)\n",
    "sizess = np.round(sizess/2).astype(int)\n",
    "print(sizess)\n",
    "np.save('./data/look_ups/reduced_sizes_clip.npy', sizess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data set into train and test sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_data = roi_data_concat\n",
    "# y_data = tensor_image_features\n",
    "\n",
    "# # print(\"Shape of x_data:\", x_data.shape)\n",
    "# # print(\"Shape of y_data:\", y_data.shape)\n",
    "\n",
    "# # tensor split data\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # print shape of train and test data\n",
    "# print(\"Shape of x_train:\", x_train.shape)\n",
    "# print(\"Shape of y_train:\", y_train.shape)\n",
    "# print(\"Shape of x_test:\", x_test.shape)\n",
    "# print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting train to be full and test to be 0.1 size of the data\n",
    "glob_roi_fmri_data = roi_data_concat.to(device)\n",
    "glob_img_clip_reps = unique_image_features.to(device)\n",
    "glob_img_names = unique_images\n",
    "glob_captions = unique_captions\n",
    "glob_captions_reps = torch.stack(unique_captions_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = roi_data_concat.to(device)\n",
    "y_train = unique_image_features.to(device)\n",
    "\n",
    "_x_train = roi_data_concat[200:].to(device)\n",
    "_y_train = unique_image_features[200:].to(device)\n",
    "\n",
    "img_names_train = unique_images\n",
    "\n",
    "x_test = roi_data_concat[:200].to(device)\n",
    "y_test = unique_image_features[:200].to(device)\n",
    "\n",
    "img_test_names = unique_images[:200]\n",
    "\n",
    "# print shape of train and test data\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"len of img_names_train:\", len(img_names_train))\n",
    "print()\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"len of img_test_names:\", len(img_test_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index \n",
    "ex_image_name = glob_img_names[0:50]\n",
    "ex_image_rep = glob_img_clip_reps[0:50].to(device)\n",
    "ex_fmri_data = x_test[0:50].to(device)\n",
    "\n",
    "indexes = [27,6,7,8,14,13]  # Example indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evalaution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion_accuracy, criterion_glove, get_ranks, get_mean_ranks, img_test_names,model_type=None):\n",
    "\tplot_ranks = []\n",
    "\taccuracy_test = 0\n",
    "\trank = 0\n",
    "\tloss_glove = 0\n",
    "\tcnt = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\toutputs_glove = None\n",
    "\t\t\tif model_type == None: \n",
    "\t\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\telif model_type == \"small\":\n",
    "\t\t\t\toutputs_glove = model(inputs)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise Exception(\"Invalid model type\")\n",
    "\t\t\tplot_ranks.extend(get_ranks(outputs_glove[:10], img_test_names[:10]))\n",
    "\t\t\taccuracy_test += criterion_accuracy(outputs_glove[:8], targets_glove[:8])\n",
    "\t\t\trank += get_mean_ranks(outputs_glove[:10], img_test_names[:10])\n",
    "\t\t\tloss_glove += criterion_glove(outputs_glove, targets_glove)\n",
    "\t\t\tcnt += 1\n",
    "\t\t\t\n",
    "\tavg_accuracy_test = accuracy_test / cnt\n",
    "\tavg_loss_glove = loss_glove / cnt\n",
    "\tavg_rank = rank / cnt\n",
    "\n",
    "\treturn avg_accuracy_test, avg_loss_glove, avg_rank, plot_ranks\n",
    "\n",
    "def plot_ranks_distribution(plot_ranks):\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.hist(plot_ranks, bins=50, color='skyblue', edgecolor='black')\n",
    "\tplt.title('Distribution of Ranks')\n",
    "\tplt.xlabel('Rank')\n",
    "\tplt.ylabel('Frequency')\n",
    "\tplt.xlim(0, 2000)  # Limit x-axis from 0 to 2000\n",
    "\tplt.grid(True)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dic_sorted_by_cosine_similarity(query,glob_names=glob_img_names,glob_rep=glob_img_clip_reps):\n",
    "\tcosine_similarities = {}\n",
    "\tfor img_name, feature in zip(glob_names, glob_rep):\t\n",
    "\t\tfeature.to(device)\n",
    "\t\tassert query.shape == torch.Size([512]), \"Query should be 512\"\n",
    "\t\tassert feature.shape == torch.Size([512]), \"Feature should be 512\"\n",
    "\t\tcosine_similarities[img_name] = torch.nn.functional.cosine_similarity(query, feature, dim=0)\n",
    "\t\tcosine_similarities[img_name] = cosine_similarities[img_name].item()\n",
    "  \n",
    "\t# get top 5 representations along with names\n",
    "\tcosine_similarities = dict(sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=True))\n",
    "  \n",
    "\treturn cosine_similarities\n",
    "\n",
    "def find_top5_img(query):\n",
    "\tcosine_similarities = get_dic_sorted_by_cosine_similarity(query)\n",
    "\ttop5_image = list(cosine_similarities.items())[:5]\n",
    "\tret_dic = {}\n",
    " \n",
    "\t# get image representations from top 5 image \n",
    "\t# print(\"Top 5 images:\")\n",
    "\tfor img_name, sim in top5_image:\n",
    "\t\t# get index of image from image names\n",
    "\t\tindex = glob_img_names.index(img_name)\n",
    "\t\t# get image feature\n",
    "\t\t# print(\"sim: \",sim)\n",
    "\t\timage_feature = glob_img_clip_reps[index]\n",
    "\t\tret_dic[img_name] = image_feature\n",
    "  \n",
    "\treturn ret_dic\n",
    "\n",
    "def evaluate_gt_feature_in_top5(y_preds, y_gts):\n",
    "\t# get top 5 images for each query\n",
    "\tcorrect_pred_top5 = 0\n",
    "\tfor y_pred, y_gt in zip(y_preds, y_gts):\n",
    "\t\tassert y_pred.shape == torch.Size([512]), \"y_pred should be 512\"\n",
    "\t\ttop5_images_reps_dic = find_top5_img(y_pred)\n",
    "  \n",
    "\t\tfound = False\n",
    "\t\tfor img_name, img_rep in top5_images_reps_dic.items():\n",
    "\t\t\t# check if y_gt is in top 5 images\n",
    "\t\t\tif 0.98 <= torch.nn.functional.cosine_similarity(y_gt, img_rep, dim=0) <= 1.1:\n",
    "\t\t\t\tfound = True\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tif found:\n",
    "\t\t\tcorrect_pred_top5 += 1\n",
    "   \n",
    "\taccuracy = correct_pred_top5 *100 / len(y_preds) \n",
    "\t# print(f\"correct_pred_top5: {correct_pred_top5}, Total: {len(y_preds)}\")\n",
    "\t# print(\"Accuracy:\", accuracy)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_closest_names(query, k=5, glob_names=glob_img_names, glob_rep=glob_img_clip_reps.to(device)):\n",
    "\tcosine_similarities = get_dic_sorted_by_cosine_similarity(query, glob_names, glob_rep)\n",
    " \n",
    "\tout = list(cosine_similarities.keys())\n",
    "\tout = out[:k]\n",
    " \n",
    "\treturn out\n",
    " \n",
    "\treturn out\n",
    "\n",
    "def get_rank_query(query, search):\n",
    "\tcosine_similarities = get_dic_sorted_by_cosine_similarity(query)\n",
    " \n",
    "\trank = 0\n",
    "\tfor img_name in cosine_similarities.keys():\n",
    "\t\trank += 1\n",
    "\t\tif img_name == search:\n",
    "\t\t\tbreak\n",
    " \n",
    "\treturn rank\n",
    "\n",
    "def get_ranks(y_preds, gt_names):\n",
    "\tranks = []\n",
    "\tfor y_pred, gt_name in zip(y_preds, gt_names):\n",
    "\t\tassert y_pred.shape == torch.Size([512]), \"y_pred should be 512\"\n",
    "\t\trank = get_rank_query(y_pred, gt_name)\n",
    "\t\tranks.append(rank)\n",
    "  \n",
    "\treturn np.array(ranks)\n",
    "\n",
    "def get_mean_ranks(y_preds, gt_names):\n",
    "\treturn np.mean(get_ranks(y_preds, gt_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_original_and_closest_image(indexes, model, glob_img_names, glob_img_clip_reps, x_test, k=1, ex_fmri_data=None, ex_image_rep=None):\n",
    "    # Create dataset and data loader for all images\n",
    "    test_dataset = TensorDataset(ex_fmri_data.to(device), ex_image_rep.to(device))\n",
    "    ex_test_loader = DataLoader(test_dataset, batch_size=len(x_test))\n",
    "    \n",
    "    # Run the model\n",
    "    for roi_data, img_clip_reps in ex_test_loader:\n",
    "        out, outputs_glove, _ = model(roi_data)\n",
    "        break\n",
    "    \n",
    "    # Iterate over each index\n",
    "    for idx in indexes:\n",
    "        # Get the closest image names for this index\n",
    "        closest_names = get_k_closest_names(outputs_glove[idx], k)\n",
    "        \n",
    "        # Load original image\n",
    "        ex_image = load_images_from_names(glob_img_names[idx])\n",
    "        \n",
    "        # Plot original and closest images\n",
    "        fig, axes = plt.subplots(1, k+1, figsize=(15, 5))\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[0].imshow(ex_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        \n",
    "        # Plot closest images\n",
    "        for i, name in enumerate(closest_names):\n",
    "            closest_img = load_images_from_names(name)\n",
    "            axes[i+1].imshow(closest_img)\n",
    "            axes[i+1].set_title(f'Closest Image Rank {i+1}')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "def print_predictions(model, indexes, x_test, \n",
    "                      glob_img_names=glob_img_names,\n",
    "                      glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t\t  ex_fmri_data=None, \n",
    "\t\t\t\t\t  ex_image_rep=None,\n",
    "\t\t\t\t\t  model_type=None):\n",
    "\t# Create dataset and data loader for all images\n",
    "\ttest_dataset = TensorDataset(ex_fmri_data.to(device), ex_image_rep.to(device))\n",
    "\tex_test_loader = DataLoader(test_dataset, batch_size=len(x_test))\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\t# Run the model\n",
    "\tfor roi_data, img_clip_reps in ex_test_loader:\n",
    "\t\tif model_type == None:\n",
    "\t\t\tout, outputs_glove, _ = model(roi_data)\n",
    "\t\telif model_type == \"small\":\n",
    "\t\t\toutputs_glove = model(roi_data)\n",
    "\t\tbreak\n",
    "\t\n",
    "\t# Iterate over each index\n",
    "\tfor idx in indexes:\n",
    "\t\t# Get the closest image names for this index\n",
    "\t\tclosest_name = get_k_closest_names(outputs_glove[idx], 1)[0]\n",
    "\t\tcloset_captions = get_k_closest_names(outputs_glove[idx], 2, glob_captions, glob_caption_reps)[:2]\n",
    "\t\t\n",
    "\t\t# Get the actual image name\n",
    "\t\tactual_name = glob_img_names[idx]\n",
    "\t\t\n",
    "\t\t# Print the predictions\n",
    "\t\tprint(f\"Actual Image: {actual_name}\")\n",
    "\t\tplt.imshow(load_images_from_names(actual_name))\n",
    "\t\tplt.show()\n",
    "\t\tprint(\"Original Caption:\")\n",
    "\t\tprint(\"\\\"| \",img_best_caption[actual_name],\" |\\\"\")\n",
    "\t\tprint(\"\\nPredictions:\")\n",
    "\t\tprint(f\"Closest Image: {closest_name}\")\n",
    "\t\tplt.imshow(load_images_from_names(closest_name))\n",
    "\t\tplt.show()\n",
    "\t\tprint(\"Caption:\")\n",
    "\t\tprint(\"\\\"| \",closet_captions[0],\" |\\\"\")\n",
    "\t\tprint(\"Gen image using rep:\",end=\" \")\n",
    "\t\tgenerate_img(closet_captions[1])\n",
    "\t\tprint(\"==\"*50,\"\\n\\n\"*4)\n",
    "\n",
    "\t\t# ==========================================\n",
    "  \n",
    "def print_predictions_one_rep(model, index, x_test, \n",
    "                      glob_img_names=glob_img_names,\n",
    "                      glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t\t  ex_fmri_data=None, \n",
    "\t\t\t\t\t  ex_image_rep=None,\n",
    "\t\t\t\t\t  model_type=None,k=2):\n",
    "\t# Create dataset and data loader for all images\n",
    "\ttest_dataset = TensorDataset(ex_fmri_data.to(device), ex_image_rep.to(device))\n",
    "\tex_test_loader = DataLoader(test_dataset, batch_size=len(x_test))\n",
    "\t\n",
    "\t# Run the model\n",
    "\tfor roi_data, img_clip_reps in ex_test_loader:\n",
    "\t\tif model_type == None:\n",
    "\t\t\tout, outputs_glove, _ = model(roi_data)\n",
    "\t\telif model_type == \"small\":\n",
    "\t\t\toutputs_glove = model(roi_data)\n",
    "\t\t\toutputs_glove.to('cpu')\n",
    "\t\tbreak\n",
    "\n",
    "\t# Get the closest image names for this index\n",
    "\tclosest_names = get_k_closest_names(outputs_glove[index].cpu(), k)\n",
    "\tcloset_captions = get_k_closest_names(outputs_glove[index].cpu(), k, glob_captions, glob_caption_reps)\n",
    "\t\n",
    " \t# Get the actual image name\n",
    "\tactual_name = glob_img_names[index]\n",
    "\t\n",
    "\t# Print the predictions\n",
    "\tprint(f\"Actual Image: {actual_name}\")\n",
    "\tplt.imshow(load_images_from_names(actual_name))\n",
    "\tplt.show()\n",
    "\t# print(\"Original Caption:\",end=\" \")\n",
    "\t# print(\"\\\"| \",img_best_caption[actual_name],\" |\\\"\")\n",
    "\tprint(\"==\"*50)\n",
    "\t\n",
    "\t# Iterate over each index\n",
    "\tfor i in range(k):\n",
    "\t\tprint(\"==\"*50)\n",
    "\t\tprint(\"\\nPredictions:\")\n",
    "\t\tprint(f\"Closest Image: {closest_names[i]}\")\n",
    "\t\tplt.imshow(load_images_from_names(closest_names[i]))\n",
    "\t\tplt.title(f\"Caption: {img_best_caption[closest_names[i]]}\")\n",
    "\t\tplt.show()\n",
    "\t\n",
    "\tfor i in range(k):\n",
    "\t\tprint(\"Caption:\",end=\" \")\n",
    "\t\tprint(\"\\\"| \",closet_captions[i],\" |\\\"\")\n",
    "  \n",
    "\t# TODO\n",
    "\tfor i in range(k):\n",
    "\t\tprint(\"Gen image using rep:\",end=\" \")\n",
    "\t\tgenerate_img(img_best_caption[closest_names[i]])\n",
    "\t\t# print(ret_out)\n",
    "\t\t# plt.imshow(np.array())\n",
    "\t\t# print(f\"Caption: {img_best_caption[closest_names[i]]}\")\n",
    "\t\t# plt.show()\n",
    "\tprint(\"==\"*50,\"\\n\\n\"*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sanity check of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 sanity check\n",
    "evaluate_gt_feature_in_top5(y_test[:5], y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank sanity check\n",
    "ranks = get_ranks(y_test[1:10], img_test_names[0:9])\n",
    "\n",
    "print(\"Ranks:\", ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for rank\n",
    "# select a random image and get its representation\n",
    "index = 1349\n",
    "index_name = unique_images[index]\n",
    "\n",
    "# laod and print img\n",
    "img = load_images_from_names(index_name)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "y_pred = unique_image_features[index]\n",
    "print(\"Shape of y_pred:\", y_pred.shape)\n",
    "\n",
    "print(img_test_names)\n",
    "print(y_test.shape)\n",
    "\n",
    "# get rank of the image from test \n",
    "closest_names = get_k_closest_names(y_pred, k=5)\n",
    "\n",
    "for i, name in enumerate(closest_names):\n",
    "    img = load_images_from_names(name)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"Rank: {i+1}, Name: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models\n",
    "\n",
    "### Model with Pretraining (Reconstruction and Classification Separately)\n",
    "\n",
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\tdef __init__(self, mean = False):\n",
    "\t\tsuper(Autoencoder, self).__init__()\n",
    "\n",
    "\t\trate = 0.4\n",
    "\t\tdense_size = 512\n",
    "\t\tglove_size = 512\n",
    "\t\treduced_size = 843\n",
    "\n",
    "\t\tsizes = np.load('./data/look_ups/sizes_clip.npy')\n",
    "\t\treduced = np.load('./data/look_ups/reduced_sizes_clip.npy')\n",
    "\n",
    "\t\tself.dense_layers = nn.ModuleList([nn.Linear(sizes[i], reduced[i]) for i in range(len(sizes))])\n",
    "\t\tself.batch_norm = nn.ModuleList([nn.BatchNorm1d(reduced[i]) for i in range(len(sizes))])\n",
    "\t\tself.dropout = nn.Dropout(rate)\n",
    "\n",
    "\t\tself.batch_norm11 = nn.BatchNorm1d(reduced_size)\n",
    "\t\tself.dropout11 = nn.Dropout(rate)\n",
    "\n",
    "\t\tself.dense1 = nn.Linear(reduced_size, dense_size)\n",
    "\t\tself.leaky_relu = nn.LeakyReLU(0.3)\n",
    "\t\tself.batch_norm1 = nn.BatchNorm1d(dense_size)\n",
    "\t\tself.dropout1 = nn.Dropout(rate)\n",
    "\n",
    "\t\tself.dense2 = nn.Linear(dense_size, glove_size)\n",
    "\t\tself.batch_norm2 = nn.BatchNorm1d(glove_size)\n",
    "\t\tself.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "\t\tself.dense3 = nn.Linear(glove_size, 180)\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\t\tself.dense4 = nn.Linear(dense_size, reduced_size)\n",
    "\t\tself.batch_norm3 = nn.BatchNorm1d(reduced_size)\n",
    "\t\tself.dropout3 = nn.Dropout(rate)\n",
    "\n",
    "\t\tself.dense_layers_transpose = nn.ModuleList([nn.Linear(reduced[i], sizes[i]) for i in range(len(sizes))])\n",
    "\t\tself.batch_norm4 = nn.ModuleList([nn.BatchNorm1d(size) for size in sizes])\n",
    "\t\t# self.dense_transpose_layer = DenseTranspose(nn.Linear(dense_size, gordon_areas * reduced_size))\n",
    "\t\t# self.dense_transpose_layers = nn.ModuleList([DenseTranspose(dense) for dense in self.dense_layers])\n",
    "\t\t# self.dense_transpose_layers = nn.ModuleList([nn.Linear(reduced_size, size) for size in sizes])\n",
    "\n",
    "\t\tself.mean = mean\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# print(1)\n",
    "\t\tbranch_outputs = []\n",
    "\t\tindex = 0\n",
    "\t\tgordon_areas = 10\n",
    "\t\tsizes = np.load('./data/look_ups/sizes_clip.npy')\n",
    "\t\treduced = np.load('./data/look_ups/reduced_sizes_clip.npy')\n",
    "\t\tfor i in range(gordon_areas):\n",
    "\t\t\tnew_index = index + sizes[i]\n",
    "\t\t\tsmall_input = x[:, index:new_index]\n",
    "\t\t\tdense_out = self.leaky_relu(self.dense_layers[i](small_input))\n",
    "\t\t\tdense_out = self.batch_norm[i](dense_out)\n",
    "\t\t\tdense_out = self.dropout(dense_out)\n",
    "\t\t\tbranch_outputs.append(dense_out)\n",
    "\t\t\tindex = new_index\n",
    "\n",
    "\t\t# tensor_branch_outputs = torch.cat(branch_outputs, dim=1)\n",
    "\n",
    "\t\t# print(tensor_branch_outputs.size())\n",
    "\n",
    "\t\tconcat = torch.cat(branch_outputs, dim=1)\n",
    "\t\t# print(concat.size())\n",
    "\t\tdense1_out = self.batch_norm11(concat)\n",
    "\t\tdense1_out = self.dropout11(dense1_out)\n",
    "\t\t# print((dense1_out).size())\n",
    "\t\tout_further = self.leaky_relu(self.dense1(dense1_out))\n",
    "\t\tout_further = self.batch_norm1(out_further)\n",
    "\t\tout_further = self.dropout1(out_further)\n",
    "\t\t# print((out_further).size())\n",
    "\t\tout_glove = self.leaky_relu(self.dense2(out_further))\n",
    "\t\tout_glove = self.batch_norm2(out_glove)\n",
    "\t\tout_glove = self.dropout2(out_glove)\n",
    "\t\t# print((out_glove).size())\n",
    "\t\tout_class = self.softmax(self.dense3(out_glove))\n",
    "\t\t# print((out_class).size())\n",
    "\t\tdense4_out = self.leaky_relu(self.dense4(out_further))\n",
    "\t\t# print(\"hiiiiii\")\n",
    "\t\tdense4_out = self.batch_norm3(dense4_out)\n",
    "\t\tdense4_out = self.dropout3(dense4_out)\n",
    "\t\t# print((dense4_out).size())\n",
    "\t\tbranch_outputs1 = []\n",
    "\t\tindex1 = 0\n",
    "\t\tfor j in range(gordon_areas):\n",
    "\t\t\tnew_index1 = index1 + reduced[j]\n",
    "\t\t\tsmall_input = dense4_out[:, index1:new_index1]\n",
    "\t\t\t# print(\"hi\")\n",
    "\t\t\t# print(small_input.size())\n",
    "\t\t\tdense_out = self.leaky_relu(self.dense_layers_transpose[j](small_input))\n",
    "\t\t\t# print(\"bye\")\n",
    "\t\t\tdense_out = self.batch_norm4[j](dense_out)\n",
    "\t\t\tbranch_outputs1.append(dense_out)\n",
    "\t\t\tindex1 = new_index1\n",
    "\t\tout = torch.cat(branch_outputs1, dim=1)\n",
    "\t\t# print((out).size())\n",
    "\t\tif not self.mean:\n",
    "\t\t\t# print(\"yayyyyy!!!\")\n",
    "\t\t\treturn out, out_glove, out_class\n",
    "\t\telse:\n",
    "\t\t\t# print(\"fuckkkkkkk\")\n",
    "\t\t\treturn out, out_glove, out_class, concat, dense1_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size_train = 64\n",
    "train_dataset = TensorDataset(x_train.to(device), y_train.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test.to(device), y_test.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=50)\n",
    "\n",
    "model = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer_specific = torch.optim.Adam([\n",
    "\t{'params': model.dense2.parameters()},\n",
    "\t{'params': model.batch_norm2.parameters()}\n",
    "], lr=0.001)\n",
    "\n",
    "loss_fn_glove = CosineProximityLoss()\n",
    "criterion_glove = CosineProximityLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "criterion_accuracy = evaluate_gt_feature_in_top5\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_autoencoder = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tout, output_glove, output_class = model(batch_data)\n",
    "\t\tloss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "\t\tloss = loss_autoencoder\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer.step()\n",
    "\t\tloss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\trunning_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\tepoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\tloss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "\trunning_loss = epoch_loss_glove + epoch_loss_autoencoder\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Glove Loss: {loss_glove:.4f}')\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_autoencoder = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer_specific.zero_grad()\n",
    "\t\tout, output_glove, output_class = model(batch_data)\n",
    "\t\tloss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "\t\tloss = loss_glove\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer_specific.step()\n",
    "\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\trunning_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\t\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\tepoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\tloss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "\trunning_loss = epoch_loss_glove + epoch_loss_autoencoder\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Glove Loss: {loss_glove:.4f}')\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy_test, avg_loss_glove, avg_rank, plot_ranks = evaluate_model(model, test_loader, criterion_accuracy, criterion_glove, get_ranks, get_mean_ranks, img_test_names)\n",
    "\n",
    "# print(accuracy_test/cnt, loss_glove/cnt, rank/cnt)\n",
    "print(\"Top 5 Accuracy:\", avg_accuracy_test)\n",
    "print(\"Glove Loss:\", avg_loss_glove)\n",
    "print(\"Mean Rank:\", avg_rank)\n",
    "\n",
    "# plot the ranks\n",
    "plot_ranks_distribution(plot_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "print_predictions(model, indexes, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions_one_rep(model, 0, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model with Pretraining (Reconstruction and Classification Together)\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size_train = 16\n",
    "train_dataset = TensorDataset(x_train.to(device), y_train.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_dataset = TensorDataset(x_test.to(device), y_test.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=50)\n",
    "\n",
    "model = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer_specific = torch.optim.Adam([\n",
    "\t{'params': model.dense2.parameters()},\n",
    "\t{'params': model.batch_norm2.parameters()}\n",
    "], lr=0.001)\n",
    "\n",
    "loss_fn_glove = CosineProximityLoss()\n",
    "criterion_glove = CosineProximityLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "criterion_accuracy = evaluate_gt_feature_in_top5\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_autoencoder = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tout, output_glove, output_class = model(batch_data)\n",
    "\t\tloss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "\t\tloss = loss_autoencoder\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\trunning_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\t\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\tepoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\tloss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "\trunning_loss = epoch_loss_glove + epoch_loss_autoencoder\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Glove Loss: {loss_glove:.4f}')\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_autoencoder = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tout, output_glove, output_class = model(batch_data)\n",
    "\t\tloss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "\t\tloss = loss_glove\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\trunning_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\t\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\tepoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\tloss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "\trunning_loss = epoch_loss_glove + epoch_loss_autoencoder\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Glove Loss: {loss_glove:.4f}')\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy_test, avg_loss_glove, avg_rank, plot_ranks = evaluate_model(model, test_loader, criterion_accuracy, criterion_glove, get_ranks, get_mean_ranks, img_test_names)\n",
    "\n",
    "# print(accuracy_test/cnt, loss_glove/cnt, rank/cnt)\n",
    "print(\"Top 5 Accuracy:\", avg_accuracy_test)\n",
    "print(\"Glove Loss:\", avg_loss_glove)\n",
    "print(\"Mean Rank:\", avg_rank)\n",
    "\n",
    "# plot the ranks\n",
    "plot_ranks_distribution(plot_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(model, indexes, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions_one_rep(model, 0, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model without Pretraining\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size_train = 128\n",
    "train_dataset = TensorDataset(x_train.to(device), y_train.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "batch_size_test = 100\n",
    "test_dataset = TensorDataset(x_test.to(device), y_test.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test)\n",
    "\n",
    "model = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn_glove = CosineProximityLoss()\n",
    "criterion_glove = CosineProximityLoss()\n",
    "loss_fn_autoencoder = CosineProximityLoss()\n",
    "criterion_accuracy = evaluate_gt_feature_in_top5\n",
    "\n",
    "epochs = 15\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\taccuracy_test = 0\n",
    "\tloss_glove = 0\n",
    "\tcnt = 0\n",
    "\tfor inputs, targets_glove in test_loader:\n",
    "\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\tloss_glove += criterion_glove(outputs_glove, targets_glove)\n",
    "\t\tcnt += 1\n",
    "print(loss_glove/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_autoencoder = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tout, output_glove, output_class = model(batch_data)\n",
    "\t\tloss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "\t\tloss_autoencoder = loss_fn_autoencoder(batch_data, out)\n",
    "\t\tloss = loss_autoencoder + loss_glove\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\trunning_loss_autoencoder += loss_autoencoder.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\t\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\tepoch_loss_autoencoder = running_loss_autoencoder / len(train_loader.dataset)\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\tout, outputs_glove, outputs_class = model(inputs)\n",
    "\t\t\tloss_glove = criterion_glove(outputs_glove, targets_glove)\n",
    "\n",
    "\trunning_loss = epoch_loss_glove + epoch_loss_autoencoder\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tprint(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Glove Loss: {loss_glove:.4f}')\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy_test, avg_loss_glove, avg_rank, plot_ranks = evaluate_model(model, test_loader, criterion_accuracy, criterion_glove, get_ranks, get_mean_ranks, img_test_names)\n",
    "\n",
    "# print(accuracy_test/cnt, loss_glove/cnt, rank/cnt)\n",
    "print(\"Top 5 Accuracy:\", avg_accuracy_test)\n",
    "print(\"Glove Loss:\", avg_loss_glove)\n",
    "print(\"Mean Rank:\", avg_rank)\n",
    "\n",
    "# plot the ranks\n",
    "plot_ranks_distribution(plot_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(model, indexes, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions_one_rep(model, 0, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Small Model\n",
    "\n",
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMRI_size = roi_data_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecSmallModel(nn.Module):\n",
    "\tdef __init__(self, rate=0.3, glove_size=512, dense_size=1000, fMRI_size=FMRI_size):\n",
    "\t\tsuper(EncDecSmallModel, self).__init__()\n",
    "\t\t\n",
    "\t\tself.dense_first = nn.Sequential(nn.Linear(fMRI_size, dense_size),nn.LeakyReLU(0.3),nn.BatchNorm1d(dense_size),nn.Dropout(rate))\n",
    "\t\tself.out_glove = nn.Linear(dense_size, glove_size)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.dense_first(x)\n",
    "\t\tout_glove = self.out_glove(x)\n",
    "\t\treturn out_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(_x_train, _y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Instantiate the model\n",
    "model = EncDecSmallModel().to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn_glove = CosineProximityLoss()\n",
    "criterion_glove = evaluate_gt_feature_in_top5\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 25\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\tmodel.train()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_loss_glove = 0.0\n",
    "\trunning_loss_class = 0.0\n",
    "\tcnt2 = 0\n",
    "\tfor batch_data, batch_glove in tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutput_glove = model(batch_data)\n",
    "\t\tloss_glove = loss_fn_glove(output_glove, batch_glove)\n",
    "\t\tloss = loss_glove\n",
    "\t\tloss.backward()\n",
    "\t\tloss.dtype\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\trunning_loss_glove += loss_glove.item() * batch_data.size(0)\n",
    "\t\tcnt2+=1\n",
    "\t\n",
    "\tepoch_loss_glove = running_loss_glove / len(train_loader.dataset)\n",
    "\n",
    "\tcnt = 0\n",
    "\tpair = 0\n",
    "\trun_acc = 0\n",
    "\tavg_cos_dist = 0\n",
    "\t\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, targets_glove in test_loader:\n",
    "\t\t\toutputs_glove = model(inputs)\n",
    "\t\t\t# Calculate accuracy for classification\n",
    "\t\t\t# print(inputs.shape,' ',targets_class.shape,' ',outputs_class.shape)\n",
    "\t\t\t# _, predicted_class = torch.max(outputs_class, 1)\n",
    "\n",
    "\t\t\t# validation loss\n",
    "\t\t\t# print(outputs_glove.shape,' ',targets_glove.shape)\n",
    "\t\t\taccuracy_test = criterion_glove(outputs_glove[:8],targets_glove[:8])\n",
    "\t\t\tcos_dist = 0\n",
    "\t\t\tfor i in range(outputs_glove.shape[0]):\n",
    "\t\t\t\tcos_dist += torch.nn.functional.cosine_similarity(outputs_glove[i],targets_glove[i],dim=0)\n",
    "\t\t\t\n",
    "\t\t\tcnt+=1\n",
    "\t\t\tpair += loss_glove\n",
    "\t\t\trun_acc += accuracy_test / outputs_glove.shape[0]\n",
    "\t\t\tavg_cos_dist += cos_dist / outputs_glove.shape[0]\n",
    "\n",
    "\t# Calculate average accuracy\n",
    "\t# average_accuracy = total_accuracy / len(validation_dataset)\n",
    "\tpair = pair /cnt\n",
    "\trunning_loss = epoch_loss_glove\n",
    "\taverage_loss = running_loss / cnt2\n",
    "\tacc = run_acc / cnt\n",
    "\tavg_cos_dist = avg_cos_dist / cnt\n",
    "\t# print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {average_loss:.4f}, Average Top 1 Accuracy: {average_a1:.4f}, Average Top 5 Accuracy: {average_a5:.4f}, Average Top 10 Accuracy: {average_a10:.4f}')\n",
    "\t# print(f'Pairwise Loss: {pair}')\n",
    "\tprint(f\"Epoch {epoch+1}/{epochs}, Glove Loss: {epoch_loss_glove}, Pairwise Loss: {pair}, Top5 Accuracy: {acc}, cosine distance from gt on avg: {avg_cos_dist}\")\n",
    "\tscheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy_test, avg_loss_glove, avg_rank, plot_ranks = evaluate_model(model, test_loader, criterion_accuracy, criterion_glove, get_ranks, get_mean_ranks, img_test_names,model_type=\"small\")\n",
    "\n",
    "# print(accuracy_test/cnt, loss_glove/cnt, rank/cnt)\n",
    "print(\"Top 5 Accuracy:\", avg_accuracy_test)\n",
    "print(\"Glove Loss:\", avg_loss_glove)\n",
    "print(\"Mean Rank:\", avg_rank)\n",
    "\n",
    "# plot the ranks\n",
    "plot_ranks_distribution(plot_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(model, indexes, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep,\n",
    "\t\t\t\t  model_type=\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions_one_rep(model, 0, x_test, \n",
    "\t\t\t\t  glob_img_names=glob_img_names,\n",
    "\t\t\t\t  glob_img_clip_reps=glob_img_clip_reps,\n",
    "\t\t\t\t  glob_captions=glob_captions,\n",
    "\t\t\t\t  glob_caption_reps=glob_captions_reps,\n",
    "\t\t\t\t  ex_fmri_data=ex_fmri_data, \n",
    "\t\t\t\t  ex_image_rep=ex_image_rep,k=5,model_type=\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
